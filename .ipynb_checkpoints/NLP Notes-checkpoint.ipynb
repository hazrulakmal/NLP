{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6922574a",
   "metadata": {},
   "source": [
    "Steps Required to model logistics regressions.\n",
    "\n",
    "* Extract useful information from the data and transform them into a set of inputs aka **features**\n",
    "* Train classify model while minimising the cost funtion\n",
    "* Make Prediction \n",
    "\n",
    "### Feature Extraction (Fequency Dictionary Mapping)\n",
    "Represent a text in  a vector of dimension |v| (Vocabulary size)\n",
    "* features are a list of words (vocabulary)\n",
    "* numerical representation. if a word exists then the corresponding feature is marked one. if the word appears twice, it is marked two and so on. \n",
    "* number of parameters, n, is equal to number of features, |v|\n",
    "\n",
    "Sparse Representations\n",
    "for a large vocabulary model, two problems arise\n",
    "* expensive computational time\n",
    "* overfiting. Model is complex (too many parameters)\n",
    "\n",
    "Vocabulary frquence vector (dictionary mapping from words to frequency) counts the number times of word appears for in either positive or negative. \n",
    "Feature extraction \n",
    "Encode three terms - bias, positive features & negative features\n",
    "positive - counts the freq of words that appear in positive vocabulary frequency vector\n",
    "negarive - counts the freq of words that appear in negative vocabulary frequecy table\n",
    "\n",
    "### Preprocessing Text\n",
    "1. Eliminate handles and URLs\n",
    "2. Tokenize the strings into words (process by which a large quantity of text is devided into smaller parts - remove duplicates, punctuations)\n",
    "3. remove stops words\n",
    "4. convert all words to lower case\n",
    "5. stemming the words - transform each word to its root words\n",
    "\n",
    "### General NLP Steps\n",
    "1. Perform preprocessing \n",
    "2. Feature extraction to convert text into numerical representation. Dictionary Frequecy Mapping (list all words in positive text and negative text sepreately). for each tweet, Extract 3 columns (bias, sum of positive words, sum of negative words). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87536",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad491",
   "metadata": {},
   "source": [
    "refer to week 1 notebook for codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321e471",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Classifier with Naive Bayes Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725dd31",
   "metadata": {},
   "source": [
    "Probability is a fundamental concept in NLP tasks. \n",
    "\n",
    "### Naive Bayes Inference Conditional Rule for Binary Classification \n",
    "For a balance dataset, product of all ratios of the probability of each word given positive class and probability of similar word given the negative class. if the value is bigger than one, the numerator probiblity > than the denominator. we classify the text as positve. \n",
    "\n",
    "$$ \\prod_{i=1}^m \\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)}$$\n",
    "\n",
    "non-balance dataset requires a prior distribution $ \\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "The problem with this approach is that some words may appear in one class but not the other. this means that the probability for the class without the word is zero and when the above approach is computed, we get a calculated value of zero - not good loss of information. So to combat this problem we use laplacian smoothing. \n",
    "\n",
    "$$ P(W^{(i)} | class ) = \\frac {freq (w^{(i)} \\cap class) + 1}{N_{class} + V_{class}} $$\n",
    "\n",
    "- $ V_{class} $ refers to number of unique words in vocabulary\n",
    "- $ N_{class} $ refers to frequency of all words in class\n",
    "\n",
    "\n",
    "As $ m $ gets larger, we will encourter numerical overflow issues - that is a problem when the number is very small for computer to store. We use the propreties of log transformation to solve this issues. \n",
    "\n",
    "To make an inference, we now use this formula\n",
    "\n",
    "$$ \\sum_{i=1}^m \\lambda(w^{(i)}) = \\sum_{i=1}^m log\\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)} $$\n",
    "\n",
    "A small ratio less than one produces negative value while ratio bigger than one produces positive value. The bigger/smaller the ratio, the bigger/smaller the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6a852",
   "metadata": {},
   "source": [
    "#### Train Naive Bayes\n",
    "\n",
    "1. Collect and annote corpus \n",
    "2. Preprocessing (lowercase, remove punctuations, url, names, remove stop words, stemming word, tokenize sentences) from raw data (messy) into a clean data. (Take a big chuck of the whole project)\n",
    "3. Compute word count $ freq(W, class) $ to produce freq table\n",
    "4. Get conditional probility of a word given the class $ P(w^{(i)}|pos) $ $ P(w^{(i)}|neg) $ using laplace smoothing\n",
    "5. Get the lambda, $ \\lambda(w^{(i)}) $ \n",
    "6. Compute logprior $ log\\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "\n",
    "### Application of Naive Bayes Model\n",
    "- Sentiment Analysis\n",
    "- Author Auntentication - given  a text, predict from which author the text is written by\n",
    "- Spam Classification\n",
    "- Information Retirieval - relevant vs irrelavant document based on keyword input\n",
    "\n",
    "\n",
    "### Assumptions it holds\n",
    "1. Independece - not true in NLP. some words appears in pair more often than the other\n",
    "2. Relative frequence of class in corpus affects the model - more positive class then the model is biased towards this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b759ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307693"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4*0.25/0.13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
