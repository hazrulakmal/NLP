{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6922574a",
   "metadata": {},
   "source": [
    "Steps Required to model logistics regressions.\n",
    "\n",
    "* Extract useful information from the data and transform them into a set of inputs aka **features**\n",
    "* Train classify model while minimising the cost funtion\n",
    "* Make Prediction \n",
    "\n",
    "### Feature Extraction (Fequency Dictionary Mapping)\n",
    "Represent a text in  a vector of dimension |v| (Vocabulary size)\n",
    "* features are a list of words (vocabulary)\n",
    "* numerical representation. if a word exists then the corresponding feature is marked one. if the word appears twice, it is marked two and so on. \n",
    "* number of parameters, n, is equal to number of features, |v|\n",
    "\n",
    "Sparse Representations\n",
    "for a large vocabulary model, two problems arise\n",
    "* expensive computational time\n",
    "* overfiting. Model is complex (too many parameters)\n",
    "\n",
    "Vocabulary frquence vector (dictionary mapping from words to frequency) counts the number times of word appears for in either positive or negative. \n",
    "Feature extraction \n",
    "Encode three terms - bias, positive features & negative features\n",
    "positive - counts the freq of words that appear in positive vocabulary frequency vector\n",
    "negarive - counts the freq of words that appear in negative vocabulary frequecy table\n",
    "\n",
    "### Preprocessing Text\n",
    "1. Eliminate handles and URLs\n",
    "2. Tokenize the strings into words (process by which a large quantity of text is devided into smaller parts - remove duplicates, punctuations)\n",
    "3. remove stops words\n",
    "4. convert all words to lower case\n",
    "5. stemming the words - transform each word to its root words\n",
    "\n",
    "### General NLP Steps\n",
    "1. Perform preprocessing \n",
    "2. Feature extraction to convert text into numerical representation. Dictionary Frequecy Mapping (list all words in positive text and negative text sepreately). for each tweet, Extract 3 columns (bias, sum of positive words, sum of negative words). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87536",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad491",
   "metadata": {},
   "source": [
    "refer to week 1 notebook for codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321e471",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Classifier with Naive Bayes Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725dd31",
   "metadata": {},
   "source": [
    "Probability is a fundamental concept in NLP tasks. \n",
    "\n",
    "### Naive Bayes Inference Conditional Rule for Binary Classification \n",
    "For a balance dataset, product of all ratios of the probability of each word given positive class and probability of similar word given the negative class. if the value is bigger than one, the numerator probiblity > than the denominator. we classify the text as positve. \n",
    "\n",
    "$$ \\prod_{i=1}^m \\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)}$$\n",
    "\n",
    "non-balance dataset requires a prior distribution $ \\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "The problem with this approach is that some words may appear in one class but not the other. this means that the probability for the class without the word is zero and when the above approach is computed, we get a calculated value of zero - not good loss of information. So to combat this problem we use laplacian smoothing. \n",
    "\n",
    "$$ P(W^{(i)} | class ) = \\frac {freq (w^{(i)} \\cap class) + 1}{N_{class} + V_{class}} $$\n",
    "\n",
    "- $ V_{class} $ refers to number of unique words in vocabulary\n",
    "- $ N_{class} $ refers to frequency of all words in class\n",
    "\n",
    "\n",
    "As $ m $ gets larger, we will encourter numerical overflow issues - that is a problem when the number is very small for computer to store. We use the propreties of log transformation to solve this issues. \n",
    "\n",
    "To make an inference, we now use this formula\n",
    "\n",
    "$$ \\sum_{i=1}^m \\lambda(w^{(i)}) = \\sum_{i=1}^m log\\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)} $$\n",
    "\n",
    "A small ratio less than one produces negative value while ratio bigger than one produces positive value. The bigger/smaller the ratio, the bigger/smaller the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174c959",
   "metadata": {},
   "source": [
    "#### Train Naive Bayes\n",
    "\n",
    "1. Collect and annote corpus \n",
    "2. Preprocessing (lowercase, remove punctuations, url, names, remove stop words, stemming word, tokenize sentences) from raw data (messy) into a clean data. (Take a big chuck of the whole project)\n",
    "3. Compute word count $ freq(W, class) $ to produce freq table\n",
    "4. Get conditional probility of a word given the class $ P(w^{(i)}|pos) $ $ P(w^{(i)}|neg) $ using laplace smoothing\n",
    "5. Get the lambda, $ \\lambda(w^{(i)}) $ \n",
    "6. Compute logprior $ log\\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "\n",
    "### Application of Naive Bayes Model\n",
    "- Sentiment Analysis\n",
    "- Author Auntentication - given  a text, predict from which author the text is written by\n",
    "- Spam Classification\n",
    "- Information Retirieval - relevant vs irrelavant document based on keyword input\n",
    "\n",
    "\n",
    "### Assumptions it holds\n",
    "1. Independece - not true in NLP. some words appears in pair more often than the other\n",
    "2. Relative frequence of class in corpus affects the model - more positive class then the model is biased towards this class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b715c",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "represent words and documents as vectors to capture the relative meanings of words in a sentence. \n",
    "some applications in machine translations, chatbox and text extraction. \n",
    "\n",
    "Two design paradigms \n",
    "- Word by word design - fix bandwith $k$ , distance for which to decide wether two words are next to one another \n",
    "- Word by Document design - keep tracks of number of times a word appears in a document in matrix form. words stored in row\n",
    "\n",
    "\n",
    "#### Measure of similiarity \n",
    "- euclidean distance on n-dimensional space $ d(\\vec a, \\vec b) = \\sqrt {\\sum_{i=1}^{n}(\\vec a_{i}- \\vec b_{i})^2} = || \\vec a - \\vec b || = norm(\\vec a - \\vec b) $\n",
    "- cosine similiarity can be a better proxy for similiarity than eucliden distance one vector is shorter than the other but they are close to one another as it is not biased on the size of corpus representations (vector length). Here the angle between them better captures the clossness between the two vectors. Recall $ \\vec a \\cdot \\vec b = \\sum_{i=1}^n ( a_{i} \\cdot  b_{i})$ so cosine metric is given by $ cos(\\beta) = \\frac {\\vec a \\cdot \\vec b}{||\\vec a|| \\cdot ||\\vec b||}$\n",
    "- two orthogonal vectors = Maximally dissimilar \n",
    "- notice that $ -1 \\leq cos(\\beta) \\leq 1 $, the higher the cosine value, the smaller the angle. Hence the closer the two vectors are. \n",
    "- if  $ cos(\\beta)$ is between -1 and 0 then the two vectors are dissmiliar. (Recall the cartesian plane cosine rule - cosine is only positive on first and fourth quardrant.)\n",
    "- if two vectors are identical then $ cos(\\beta) = 1$ \n",
    "\n",
    "We use measure of similiarity to predict closest meaning word for a given a word. The catch here is to represent the vector space where the word representations capture the relative meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f33eb",
   "metadata": {},
   "source": [
    "#### Predicting rela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adccef3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.array([1,2,3])\n",
    "np.array(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "333d0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n",
      "6.4031242374328485\n",
      "12.083045973594572\n",
      "0.08512565307587484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "turkey = [3,1]\n",
    "ankara = [9,1]\n",
    "japan = [4,3]\n",
    "usa = [5,6]\n",
    "wash = [10, 5]\n",
    "\n",
    "v1 = [1,2,3]\n",
    "v2 = [4,7,2]\n",
    "v3 = [3,1,4]\n",
    "v4 = [1,0,-1]\n",
    "v5 = [2,8,1]\n",
    "\n",
    "def euclidean(array1, array2):\n",
    "    return np.linalg.norm(np.array(array2)-np.array(array1))\n",
    "\n",
    "def cosine(array1, array2):\n",
    "    array1 = np.array(array1)\n",
    "    array2 = np.array(array2)\n",
    "    return (np.dot(array1, array2))/(np.linalg.norm(array1)*np.linalg.norm(array2))\n",
    "\n",
    "city = np.array(usa) - np.array(wash)\n",
    "country = np.array(ankara) + city\n",
    "print(euclidean(japan, country))\n",
    "print(euclidean(turkey, country))\n",
    "\n",
    "distance = np.array(v1) - np.array(v2)\n",
    "desired = distance + np.array(v3)\n",
    "print(euclidean(v1, desired))\n",
    "print(euclidean(v2, desired))\n",
    "print(cosine(v4,v5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6eed7",
   "metadata": {},
   "source": [
    "### PCA \n",
    "Dimensionality reduction technique that projects n dimensional space to a smaller dimension. \n",
    "\n",
    "Application\n",
    "- reduce n dimensional space into two dimension and plot them on a 2-D graph to see where the data points are relative to others. \n",
    "\n",
    "How it works?\n",
    "1. Find the eigenvector and eigenvalues of the matrix\n",
    "2. Eigenvector gives the direction of uncorrelated features\n",
    "3. Eigenvalues : amount of information retained by new features which tells how much variance there is in the vector. \n",
    "\n",
    "Steps\n",
    "1. mean normalize the data\n",
    "2. find the covariance matrix\n",
    "3. find the eigenvalues and eigenvectos of the covariance matrix\n",
    "4. rearrange the eigenvectors such that its eigenvalues are in deacreasing order. \n",
    "5. take a subset of the first n eigenvectors and multiply them with the normalize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d969875",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "1. Word embeddings in English (X) and Word embeddings in France (Y)\n",
    "2. To find the relationshipe between vector space X and Y, we need to find the maxtrix R, where XR $ \\approx $ Y\n",
    "\n",
    "Finding R using  frobenius norm\n",
    "1. initiate loop Loss = $|| XR - Y ||_F$\n",
    "2. Minimise Loss by taking its derivative $ g = \\frac{d}{dR}$Loss\n",
    "3. Update R = R - $\\alpha \\times g$ where $ \\alpha $ is a learning rate\n",
    "4. Stop when Loss falls within a certain threshold\n",
    "\n",
    "#### Frobenius norm, \n",
    "$A$ is m x n matrix\n",
    "$$ ||A||_F = \\sqrt {\\sum^m_{i=1} \\sum^n_{j=1} |\\alpha_{ij}|^2}$$\n",
    "\n",
    "#### Hast Table & Hash Function\n",
    "\n",
    "A data structure used to query a data from a table that runs constant time $O(1)$ is faster than a linear search $O(n)$\n",
    "Hash Function is a function that gives a hash value (key).\n",
    "\n",
    "A simple hash table that stores a list of number in n buckets is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d13eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.14142842854285\n",
      "7.14142842854285\n"
     ]
    }
   ],
   "source": [
    "#frobenium norm \n",
    "\n",
    "v = np.array([[1,3], [4,5]])\n",
    "\n",
    "print(np.sqrt(np.sum(np.square(v))))\n",
    "print(np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123be5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [1],\n",
       " 2: [22, 12],\n",
       " 3: [3, 13],\n",
       " 4: [44],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [77],\n",
       " 8: [88],\n",
       " 9: [9]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_function(value, n_buckets):\n",
    "    return value % n_buckets\n",
    "\n",
    "def basic_hash_table(values, n_buckets):\n",
    "    \"\"\" values : a list of elements \n",
    "        n_buckets : a scalar\n",
    "    \"\"\"\n",
    "    hash_table = {i:[] for i in range(n_buckets)}\n",
    "    for value in values:\n",
    "        hash_value = hash_function(value, n_buckets)\n",
    "        hash_table[hash_value].append(value)\n",
    "    return hash_table\n",
    "\n",
    "list_num = [1,44,22,77,3,88,9,13,12]\n",
    "\n",
    "table = basic_hash_table(list_num, 10)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f554088",
   "metadata": {},
   "source": [
    "#### Locality sensitive hashing\n",
    "used to reduce **the computational cost** of finding k-neareast neighbour in high dimensional space. \n",
    "a hash function that takes into account the relative location of a vector in a vector space to build up the hash table. \n",
    "\n",
    "1. Given a plane, find a normal vector perpendicular to the given plane.\n",
    "2. Find the dot product of a vector representing the data (need to be transposed) and the normal vector\n",
    "3. Extract the resulting vector sign to decide wether the data is below or above the plane (location of a data point relative to a plane). \n",
    "4. Generalise this idea into multiple planes \n",
    "\n",
    "#### Multiple plane generalisation\n",
    "given a point denoted by $v$, we can run in on several planes, $P_1,P_2,P_3$. If the sign of $P_1v^T$ is posivte, then we label $h_1$ as 1 while 0 if it is negative. A hash function is created such that \n",
    "$$hashvalue = \\sum_{i=0}^H 2^i \\times h_{i+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9d70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def side_of_planes(P, v):\n",
    "    \"\"\"\n",
    "        P, V both need to be a vector\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(P, v.T)\n",
    "    sign = np.sign(dot_product)\n",
    "    sign_scalar = np.asscalar(sign)\n",
    "    return sign_scalar\n",
    "\n",
    "def hash_multiple_plane(Ps, v):\n",
    "    \"\"\" Ps : a list of multiple plane\n",
    "        v : vector representing a data point\n",
    "    \"\"\"\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(Ps):\n",
    "        h_sign = side_of_planes(P, v)\n",
    "        h_i = 1 if h_sign >= 0 else 0\n",
    "        hash_value += 2**i * h_i\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67724f42",
   "metadata": {},
   "source": [
    "#### Approximate Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e05aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30431584,  0.4105472 ,  0.71611908,  0.49633876],\n",
       "       [ 1.22778007,  1.55515613, -0.06870154, -0.67537823],\n",
       "       [-2.4299803 , -1.07048808,  0.41357607,  1.04544117]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dimensions = 2\n",
    "num_planes =3 \n",
    "\n",
    "random_plane_matrix = np.random.normal(size =(num_planes, n_dimensions))\n",
    "\n",
    "def side_of_planes(Ps, v):\n",
    "    dotprod = np.dot(Ps, v.T)\n",
    "    sign = np.sign(dotprod)\n",
    "    return sign\n",
    "\n",
    "v = np.array([[1,2]])\n",
    "print(v.shape)\n",
    "side_of_planes(random_plane_matrix, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
