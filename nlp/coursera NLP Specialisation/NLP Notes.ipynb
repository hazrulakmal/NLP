{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6922574a",
   "metadata": {},
   "source": [
    "Steps Required to model logistics regressions.\n",
    "\n",
    "* Extract useful information from the data and transform them into a set of inputs aka **features**\n",
    "* Train classify model while minimising the cost funtion\n",
    "* Make Prediction \n",
    "\n",
    "### Feature Extraction (Fequency Dictionary Mapping)\n",
    "Represent a text in  a vector of dimension |v| (Vocabulary size)\n",
    "* features are a list of words (vocabulary)\n",
    "* numerical representation. if a word exists then the corresponding feature is marked one. if the word appears twice, it is marked two and so on. \n",
    "* number of parameters, n, is equal to number of features, |v|\n",
    "\n",
    "Sparse Representations\n",
    "for a large vocabulary model, two problems arise\n",
    "* expensive computational time\n",
    "* overfiting. Model is complex (too many parameters)\n",
    "\n",
    "Vocabulary frquence vector (dictionary mapping from words to frequency) counts the number times of word appears for in either positive or negative. \n",
    "Feature extraction \n",
    "Encode three terms - bias, positive features & negative features\n",
    "positive - counts the freq of words that appear in positive vocabulary frequency vector\n",
    "negarive - counts the freq of words that appear in negative vocabulary frequecy table\n",
    "\n",
    "### Preprocessing Text\n",
    "1. Eliminate handles and URLs\n",
    "2. Tokenize the strings into words (process by which a large quantity of text is devided into smaller parts - remove duplicates, punctuations)\n",
    "3. remove stops words\n",
    "4. convert all words to lower case\n",
    "5. stemming the words - transform each word to its root words\n",
    "\n",
    "### General NLP Steps\n",
    "1. Perform preprocessing \n",
    "2. Feature extraction to convert text into numerical representation. Dictionary Frequecy Mapping (list all words in positive text and negative text sepreately). for each tweet, Extract 3 columns (bias, sum of positive words, sum of negative words). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87536",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad491",
   "metadata": {},
   "source": [
    "refer to week 1 notebook for codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321e471",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Classifier with Naive Bayes Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725dd31",
   "metadata": {},
   "source": [
    "Probability is a fundamental concept in NLP tasks. \n",
    "\n",
    "### Naive Bayes Inference Conditional Rule for Binary Classification \n",
    "For a balance dataset, product of all ratios of the probability of each word given positive class and probability of similar word given the negative class. if the value is bigger than one, the numerator probiblity > than the denominator. we classify the text as positve. \n",
    "\n",
    "$$ \\prod_{i=1}^m \\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)}$$\n",
    "\n",
    "non-balance dataset requires a prior distribution $ \\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "The problem with this approach is that some words may appear in one class but not the other. this means that the probability for the class without the word is zero and when the above approach is computed, we get a calculated value of zero - not good loss of information. So to combat this problem we use laplacian smoothing. \n",
    "\n",
    "$$ P(W^{(i)} | class ) = \\frac {freq (w^{(i)} \\cap class) + 1}{N_{class} + V_{class}} $$\n",
    "\n",
    "- $ V_{class} $ refers to number of unique words in vocabulary\n",
    "- $ N_{class} $ refers to frequency of all words in class\n",
    "\n",
    "\n",
    "As $ m $ gets larger, we will encourter numerical overflow issues - that is a problem when the number is very small for computer to store. We use the propreties of log transformation to solve this issues. \n",
    "\n",
    "To make an inference, we now use this formula\n",
    "\n",
    "$$ \\sum_{i=1}^m \\lambda(w^{(i)}) = \\sum_{i=1}^m log\\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)} $$\n",
    "\n",
    "A small ratio less than one produces negative value while ratio bigger than one produces positive value. The bigger/smaller the ratio, the bigger/smaller the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174c959",
   "metadata": {},
   "source": [
    "#### Train Naive Bayes\n",
    "\n",
    "1. Collect and annote corpus \n",
    "2. Preprocessing (lowercase, remove punctuations, url, names, remove stop words, stemming word, tokenize sentences) from raw data (messy) into a clean data. (Take a big chuck of the whole project)\n",
    "3. Compute word count $ freq(W, class) $ to produce freq table\n",
    "4. Get conditional probility of a word given the class $ P(w^{(i)}|pos) $ $ P(w^{(i)}|neg) $ using laplace smoothing\n",
    "5. Get the lambda, $ \\lambda(w^{(i)}) $ \n",
    "6. Compute logprior $ log\\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "\n",
    "### Application of Naive Bayes Model\n",
    "- Sentiment Analysis\n",
    "- Author Auntentication - given  a text, predict from which author the text is written by\n",
    "- Spam Classification\n",
    "- Information Retirieval - relevant vs irrelavant document based on keyword input\n",
    "\n",
    "\n",
    "### Assumptions it holds\n",
    "1. Independece - not true in NLP. some words appears in pair more often than the other\n",
    "2. Relative frequence of class in corpus affects the model - more positive class then the model is biased towards this class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b715c",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "represent words and documents as vectors to capture the relative meanings of words in a sentence. \n",
    "some applications in machine translations, chatbox and text extraction. \n",
    "\n",
    "Two design paradigms \n",
    "- Word by word design - fix bandwith $k$ , distance for which to decide wether two words are next to one another \n",
    "- Word by Document design - keep tracks of number of times a word appears in a document in matrix form. words stored in row\n",
    "\n",
    "\n",
    "#### Measure of similiarity \n",
    "- euclidean distance on n-dimensional space $ d(\\vec a, \\vec b) = \\sqrt {\\sum_{i=1}^{n}(\\vec a_{i}- \\vec b_{i})^2} = || \\vec a - \\vec b || = norm(\\vec a - \\vec b) $\n",
    "- cosine similiarity can be a better proxy for similiarity than eucliden distance one vector is shorter than the other but they are close to one another as it is not biased on the size of corpus representations (vector length). Here the angle between them better captures the clossness between the two vectors. Recall $ \\vec a \\cdot \\vec b = \\sum_{i=1}^n ( a_{i} \\cdot  b_{i})$ so cosine metric is given by $ cos(\\beta) = \\frac {\\vec a \\cdot \\vec b}{||\\vec a|| \\cdot ||\\vec b||}$\n",
    "- two orthogonal vectors = Maximally dissimilar \n",
    "- notice that $ -1 \\leq cos(\\beta) \\leq 1 $, the higher the cosine value, the smaller the angle. Hence the closer the two vectors are. \n",
    "- if  $ cos(\\beta)$ is between -1 and 0 then the two vectors are dissmiliar. (Recall the cartesian plane cosine rule - cosine is only positive on first and fourth quardrant.)\n",
    "- if two vectors are identical then $ cos(\\beta) = 1$ \n",
    "\n",
    "We use measure of similiarity to predict closest meaning word for a given a word. The catch here is to represent the vector space where the word representations capture the relative meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f33eb",
   "metadata": {},
   "source": [
    "#### Predicting rela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333d0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n",
      "6.4031242374328485\n",
      "12.083045973594572\n",
      "0.08512565307587484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "turkey = [3,1]\n",
    "ankara = [9,1]\n",
    "japan = [4,3]\n",
    "usa = [5,6]\n",
    "wash = [10, 5]\n",
    "\n",
    "v1 = [1,2,3]\n",
    "v2 = [4,7,2]\n",
    "v3 = [3,1,4]\n",
    "v4 = [1,0,-1]\n",
    "v5 = [2,8,1]\n",
    "\n",
    "def euclidean(array1, array2):\n",
    "    return np.linalg.norm(np.array(array2)-np.array(array1))\n",
    "\n",
    "def cosine(array1, array2):\n",
    "    array1 = np.array(array1)\n",
    "    array2 = np.array(array2)\n",
    "    return (np.dot(array1, array2))/(np.linalg.norm(array1)*np.linalg.norm(array2))\n",
    "\n",
    "city = np.array(usa) - np.array(wash)\n",
    "country = np.array(ankara) + city\n",
    "print(euclidean(japan, country))\n",
    "print(euclidean(turkey, country))\n",
    "\n",
    "distance = np.array(v1) - np.array(v2)\n",
    "desired = distance + np.array(v3)\n",
    "print(euclidean(v1, desired))\n",
    "print(euclidean(v2, desired))\n",
    "print(cosine(v4,v5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6eed7",
   "metadata": {},
   "source": [
    "### PCA \n",
    "Dimensionality reduction technique that projects n dimensional space to a smaller dimension. \n",
    "\n",
    "Application\n",
    "- reduce n dimensional space into two dimension and plot them on a 2-D graph to see where the data points are relative to others. \n",
    "\n",
    "How it works?\n",
    "1. Find the eigenvector and eigenvalues of the matrix\n",
    "2. Eigenvector gives the direction of uncorrelated features\n",
    "3. Eigenvalues : amount of information retained by new features which tells how much variance there is in the vector. \n",
    "\n",
    "Steps\n",
    "1. mean normalize the data\n",
    "2. find the covariance matrix\n",
    "3. find the eigenvalues and eigenvectos of the covariance matrix\n",
    "4. rearrange the eigenvectors such that its eigenvalues are in deacreasing order. \n",
    "5. take a subset of the first n eigenvectors and multiply them with the normalize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d969875",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "1. Word embeddings in English (X) and Word embeddings in France (Y)\n",
    "2. To find the relationshipe between vector space X and Y, we need to find the maxtrix R, where XR $ \\approx $ Y\n",
    "\n",
    "Finding R using  frobenius norm\n",
    "1. initiate loop Loss = $|| XR - Y ||_F$\n",
    "2. Minimise Loss by taking its derivative $ g = \\frac{d}{dR}$Loss\n",
    "3. Update R = R - $\\alpha \\times g$ where $ \\alpha $ is a learning rate\n",
    "4. Stop when Loss falls within a certain threshold\n",
    "\n",
    "#### Frobenius norm, \n",
    "$A$ is m x n matrix\n",
    "$$ ||A||_F = \\sqrt {\\sum^m_{i=1} \\sum^n_{j=1} |\\alpha_{ij}|^2}$$\n",
    "\n",
    "#### Hast Table & Hash Function\n",
    "\n",
    "A data structure used to query a data from a table that runs constant time $O(1)$ is faster than a linear search $O(n)$\n",
    "Hash Function is a function that gives a hash value (key).\n",
    "\n",
    "A simple hash table that stores a list of number in n buckets is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d13eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.14142842854285\n",
      "7.14142842854285\n"
     ]
    }
   ],
   "source": [
    "#frobenium norm \n",
    "\n",
    "v = np.array([[1,3], [4,5]])\n",
    "\n",
    "print(np.sqrt(np.sum(np.square(v))))\n",
    "print(np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123be5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [1],\n",
       " 2: [22, 12],\n",
       " 3: [3, 13],\n",
       " 4: [44],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [77],\n",
       " 8: [88],\n",
       " 9: [9]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_function(value, n_buckets):\n",
    "    return value % n_buckets\n",
    "\n",
    "def basic_hash_table(values, n_buckets):\n",
    "    \"\"\" values : a list of elements \n",
    "        n_buckets : a scalar\n",
    "    \"\"\"\n",
    "    hash_table = {i:[] for i in range(n_buckets)}\n",
    "    for value in values:\n",
    "        hash_value = hash_function(value, n_buckets)\n",
    "        hash_table[hash_value].append(value)\n",
    "    return hash_table\n",
    "\n",
    "list_num = [1,44,22,77,3,88,9,13,12]\n",
    "\n",
    "table = basic_hash_table(list_num, 10)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f554088",
   "metadata": {},
   "source": [
    "#### Locality sensitive hashing\n",
    "used to reduce **the computational cost** of finding k-neareast neighbour in high dimensional space. \n",
    "a hash function that takes into account the relative location of a vector in a vector space to build up the hash table. \n",
    "\n",
    "1. Given a plane, find a normal vector perpendicular to the given plane.\n",
    "2. Find the dot product of a vector representing the data (need to be transposed) and the normal vector\n",
    "3. Extract the resulting vector sign to decide wether the data is below or above the plane (location of a data point relative to a plane). \n",
    "4. Generalise this idea into multiple planes \n",
    "\n",
    "#### Multiple plane generalisation\n",
    "given a point denoted by $v$, we can run in on several planes, $P_1,P_2,P_3$. If the sign of $P_1v^T$ is posivte, then we label $h_1$ as 1 while 0 if it is negative. A hash function is created such that \n",
    "$$hashvalue = \\sum_{i=0}^H 2^i \\times h_{i+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9d70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def side_of_planes(P, v):\n",
    "    \"\"\"\n",
    "        P, V both need to be a vector\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(P, v.T)\n",
    "    sign = np.sign(dot_product)\n",
    "    sign_scalar = np.asscalar(sign)\n",
    "    return sign_scalar\n",
    "\n",
    "def hash_multiple_plane(Ps, v):\n",
    "    \"\"\" Ps : a list of multiple plane\n",
    "        v : vector representing a data point\n",
    "    \"\"\"\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(Ps):\n",
    "        h_sign = side_of_planes(P, v)\n",
    "        h_i = 1 if h_sign >= 0 else 0\n",
    "        hash_value += 2**i * h_i\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67724f42",
   "metadata": {},
   "source": [
    "#### Approximate Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e05aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30431584,  0.4105472 ,  0.71611908,  0.49633876],\n",
       "       [ 1.22778007,  1.55515613, -0.06870154, -0.67537823],\n",
       "       [-2.4299803 , -1.07048808,  0.41357607,  1.04544117]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dimensions = 2\n",
    "num_planes =3 \n",
    "\n",
    "random_plane_matrix = np.random.normal(size =(num_planes, n_dimensions))\n",
    "\n",
    "def side_of_planes(Ps, v):\n",
    "    dotprod = np.dot(Ps, v.T)\n",
    "    sign = np.sign(dotprod)\n",
    "    return sign\n",
    "\n",
    "v = np.array([[1,2]])\n",
    "print(v.shape)\n",
    "side_of_planes(random_plane_matrix, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d604c",
   "metadata": {},
   "source": [
    "## Autocorrect and Minimum Edit Distance\n",
    "\n",
    "A genenral step to do autocorrect \n",
    "1. identify misspelled word - can check wether the word is in dictionary. there's more sophisicated technique that identifies missplled word such as looking at the word surrounding it to understand the context of a sentence. \n",
    "2. comupte string n edit distance away. type of edits are insert, delete, swap and replace. \n",
    "3. filter candidates (only include real words)\n",
    "4. find word probabibilities (choose a word that is the most likely to occur in the context) number of words occuring in corpus devided by the total number of words in the corpus. \n",
    "\n",
    "## Part of speech tagging\n",
    "\n",
    "tag each word with its category. ex something(noun) play (verb) why(adverb)\n",
    "\n",
    "this technique is suetable for tasks such as identifying named entities, speech recognition \n",
    "\n",
    "\n",
    "## Markov Chain\n",
    "type of stochastic model that describes a sequence of possible events. to get the probability of each event, they only need probability of the previous event. \n",
    "\n",
    "Stochastic - random\n",
    "\n",
    "Morkov chain consits of n states \n",
    "Transition matrix (transition probilities) that keep tracks of the probabilities of going from one state to another state is n+1 by n matrix.\n",
    "\n",
    "Populate transition matrix\n",
    "1. find the occurances of tag pairs $ C(t_{i-1}, t_i)$\n",
    "2. calculate the probabilities of the count\n",
    "$$ P(t_i|t_{i-1}) = \\frac {C(t_{i-1}, t_i)}{\\sum_{j=1}^n C(t_{i-1}, t_j)} $$\n",
    "\n",
    "number of times $t_i , t_{i-1}$ show up next to each other devided by the total number of times $t_{i-1}$ shows up in the corpus.\n",
    "\n",
    "Transition matrix smoothing $ \\epsilon $ is arbitrarily small value\n",
    "$$ P(t_i|t_{i-1}) = \\frac {C(t_{i-1}, t_i) + \\epsilon}{\\sum_{j=1}^N C(t_{i-1}, t_j) + N \\cdot \\epsilon} $$\n",
    "\n",
    "in the real word, one might not want to add $ \\epsilon $ to the first row of the matrix because that means we are allowing any kind of part of speach to begin a sentence. that's not true for certain POS such as punctuation. smoothing is important to avoid devision by zero in the case where POS does not appear in the corpus and better generalisation. some POS tags may not appear in the corpus but it does possible to happen. \n",
    "\n",
    "## Hiden Markov Chain\n",
    "\n",
    "refers to states that are unobservable(unseen)\n",
    "make use of emission matrix that gives probabilities of going from one state to a spesific word. for example given that you are in verb state, you can go to other word with a certain probibility. the middle state wont be exits so it is a nxn matrix.\n",
    "\n",
    "## Vertibi Algorithm\n",
    "1. identify the word in a sentece\n",
    "2. find the hidden state for the word and identify the path\n",
    "3. choose the path with the highest joint probability (between emission and transition probabilities).\n",
    "4. caculate the sequence path probability by taking the product of all the joint probabilities identified. \n",
    "\n",
    "Let A be Transition matrix, B be Emission matrix, C and D (both nxk matrix) be auxilary matrix where C keeps tracks of the probabilities of the whole sequence and D keeps track of the path in the markov chain, K is number of words in a sentence. \n",
    "\n",
    "1. Initialisation of C and D matrix\n",
    "    -  $ c_{i,1} = a_{1,i} * b_{i,cindex_(w_i)}$  &  $d_{i,1} = 0$\n",
    "2. Forward pass (At each step, you calculate the probability of each path happening and the best paths up to that point.)\n",
    "    -  $ c_{i, j} = \\max_{k}c_{k,j-1} * a_{k,i} * b_{i, cindex_(w_i)}$\n",
    "    -  $ c_{i, j} = \\argmax_{k}c_{k,j-1} * a_{k,i} * b_{i, cindex_(w_i)}$\n",
    "3. Backward pass (This allows you to find the best path with the highest probabilities)\n",
    "    -  $ s =  \\argmax_i c_{i,K} $\n",
    "    -  once s is identified, we can tranverse the path in matrix D in reverse order since each cell indicates the previous state the path is taking. \n",
    "\n",
    "\n",
    "## Autocomplete language model\n",
    "\n",
    "using probabilities to predict the next word given the first n-words. the model can also be used for auto-correct task and a search suggesting tools. \n",
    "\n",
    "Skills developed in this\n",
    "1. process a text corprus to N-gram language model\n",
    "2. Handle out of vocabulary words\n",
    "3. implement smoothing for previously unseen n-grams\n",
    "4. language model evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c92e7",
   "metadata": {},
   "source": [
    "1. Creating a language model\n",
    "2. Dealing with out of vocabulary words\n",
    "3. Improve model (more generalisable) with smoothing. \n",
    "\n",
    "## N-gram and Probabilities\n",
    "A sequence of N words/characters\n",
    "1. Unigrams a single word appears in the corpus\n",
    "2. Bigrams two words appear next to one another in the corpus\n",
    "\n",
    "Note on notations\n",
    "- $w_1^m = w_1 w_2 w_3 \\dots w_m$\n",
    "- $w_{m-2}^m = w_{m-2} w_{m-1} w_{m}$\n",
    "\n",
    "Capture the set of words from the value of the subscript to the value of the superscript. for example \"I love machine learning\", $ w_2^3$ = \"love machine\"\n",
    "\n",
    "N-gram probability \n",
    "$$ P(w_N|w_1^{N-1}) = \\frac{P(w_N, w_1^{N-1})}{P(w_1^{N-1})} =\\frac {c(w_1^{N-1}, w_N)}{c(w_1^{N-1})} = \\frac {c(w_1^N)}{c(w_1^{N-1})}$$\n",
    "\n",
    "problem with the probability calculation above is that as the sentence length get lengthy, the probability value tends to approach arbitraly small value (not good for comupter) so we use Markov assumption which indicates only the last word matters so probability of a sentence can be calculated as, for bigram model \n",
    "$$ P(w_1^N) \\approx \\prod_{i=1}^N P(w_i | w_{i-1})$$\n",
    "\n",
    "we need to adjust the starting and ending of a sentece so that we dont undercount. To generalize to an N-gram language model, you can add N-1 start tokens **\\<s>**. \n",
    "\n",
    "For the end of sentence token **\\</s>**, you only need one even if it is an N-gram. Here is an example: \n",
    "\n",
    "N-gram language model\n",
    "- probability count algorithm that avoids undeflow when multplying a lot of values ranging from 0 to 1\n",
    "\n",
    "1. count matrix rows correspond to unique corpus N-1 grams and columns correspond to unique corpus word\n",
    "2. probility matrix - devide each cell with the sum of all values for each row in the count matrix $sum(row) = c(w_{n-N+1}^{n-1})$ each cell formula is then given by $ \\frac {c(w_{n-N+1}^{n-1}, w_n)}{sum(row)}$\n",
    "3. to avoid numerical underflow (computer not good in storing small number hence prone to error), we use log\n",
    "\n",
    "### Generative language model\n",
    "ex bigram model based on highest probility\n",
    "1. choose sentence start\n",
    "2. choose next bigram starting with previous word\n",
    "3. continue until \\<s> is picked\n",
    "\n",
    "Language model evaluation\n",
    "1. split data into train, validation and test set. small corpus 8:1:1 while large corpus 98:1:1\n",
    "2. persplexity score. good model generally have 20-60 score  ($log_2$ perplexity between 4.3-5.9). it differentiates the text wether it is written by a human or a simple program choosing words at random. \n",
    "\n",
    "- $ W $ = test set containing m sentences s,\n",
    "- $ s_i = $ i-th sentence in the test set, each ending with \\</s>\n",
    "- m = the number of all words in the entire test set w excluding \\<s>\n",
    "\n",
    "General formula\n",
    "$$ PP(W) = P(s_1, s_1, \\dots , s_m)^{\\frac {1}{m}}$$\n",
    "\n",
    "bigram formula\n",
    "$$ PP(W) = \\sqrt[m] { \\prod_{i=1}^m \\frac {1}{p(w_i|w_{i-1})}} \\tag{1}$$\n",
    "equivalently\n",
    "$$ log_2PP(W) = - \\frac {1}{m} \\sum_{i=1}^m log_2((p(w_{i}|w_{i-1}))) \\tag{2}$$\n",
    "\n",
    "### Dealing with unknown word encounter in a test set.\n",
    "In some applications of language model, one may encounter a word that does not appear in the training.this word is refered to as Out-of-vocabulary(OOV). replace every unknown word with \\<unk>. \n",
    "1. create a vocabulary V\n",
    "2. replace any word in corpus and not in V by \\<unk>\n",
    "3. count the probabilities with \\<unk> just like any other words the corpus\n",
    "\n",
    "How to create a vocabluary V?\n",
    "- use the same training corpus but applies certain criteria to filter words worth being vocabluary\n",
    "    1. min word freq f. word appears more than f times are included\n",
    "    2. limit |V| size and include the top |v| word in term of frequency\n",
    "\n",
    "- carefull with the vocabulary assessment. frequent \\<unk> may result in better perplexity score but contains a lot of \\<unk> not preferable\n",
    "- only compare languange models perplexity score with the same V\n",
    "\n",
    "### smoothing\n",
    "$ P(w_N|w_1^{N-1}) = \\frac {c(w_1^{N-1}, w_N)}{c(w_1^{N-1})}$ can be 0  if a count of an n-gram is zero or $w_n$ does not appear in V\n",
    "1. add-one(laplacian)/add-k smoothing\n",
    "    - $\\frac {c(w_1^{N-1}, w_N) + 1}{c(w_1^{N-1}) + N}$ (add-one) works if the corpus is large enough to make adding one is negligable \n",
    "    - $\\frac {c(w_1^{N-1}, w_N) + k}{c(w_1^{N-1}) + K*N}$ (add-k) works if corpus is too big that addinng one to the numerator make no difference to the smoothing at all\n",
    "2. backoff\n",
    "    - Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n",
    "3. interpolation\n",
    "    - introducing lambda for each conditional probability. the sum of all lamdas must equal to 1. lambda can be derived by optimising the expected total conditional probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca335e",
   "metadata": {},
   "source": [
    "Word embeddings (word vectos)\n",
    "- learn how word vectors are created\n",
    "\n",
    "word embeddings applications\n",
    "- sentiment analysis\n",
    "- customer feedback classification\n",
    "- semantic analogies and similiarity\n",
    "- machine translation \n",
    "- information extraction\n",
    "- question-answering \n",
    "\n",
    "Basic word representations\n",
    "1. integers\n",
    "2. one hot encoding, pros: simple and no implied ordering. cons: sparse matrix requires extra space & encode no meanigful meaning.\n",
    "3. word embeddings\n",
    "\n",
    "How to create word embeddings\n",
    "to create one, we always need a corpus of text and an embedding method.\n",
    "\n",
    "1. the corpus of text used is important to capture the semantic meaning between words. Hence, the context of a text is importantwhen creating word embeddings.\n",
    "2. there are many possible methods to learn word embeddings. A machine learning model performs learning task and the by-product of this task are the word embeddings.\n",
    "\n",
    "When training a word vector, there are parameters to finetune (the dimension of the word vector - the higher the dimension, the more nauche the word vector can capture but at a cost of computational cost)\n",
    "\n",
    "### Word embedding method\n",
    "**Classical Methods**\n",
    "1. word2vec (Google, 2013)\n",
    "    - Continuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\n",
    "    - Continuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\n",
    "2. Global Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus's word co-occurrence matrix,  similar to the count matrix you’ve used before.\n",
    "3. fastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n",
    "\n",
    "**Deep learning, contextual embeddings**\n",
    " \n",
    " In these more advanced models, words have different embeddings depending on their context. You can download pre-trained embeddings for the following models. \n",
    "\n",
    "1. BERT (Google, 2018):\n",
    "2. ELMo (Allen Institute for AI, 2018)\n",
    "3. GPT-2 (OpenAI, 2018)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659385da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b1dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i', 'am']\thappy\n",
      "['am', 'happy', 'i', 'am', 'learning']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "#sliding window funtion\n",
    "\n",
    "def sliding_window(words, C):\n",
    "    \"\"\" collect center word and context words in a list\n",
    "    Args:\n",
    "        word (list): a list of tokenized words\n",
    "        C (int): the number of words surrounding the center word (context size)\n",
    "    \"\"\"\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_word = words[(i-C):i] + words[(i+1):(i+C+i)]\n",
    "        yield context_word, center_word #data generator. a function that keeps giving us data in small batches\n",
    "        i += 1\n",
    "\n",
    "for x, y in sliding_window(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77ab8c",
   "metadata": {},
   "source": [
    "## NN architecture \n",
    "\n",
    "input layers -> calculate the weighted average of input values + bias and feed in to an activation function -> hidden layers input\n",
    "V x 1\n",
    "N x V weighting matrix between input and hidden layers\n",
    "N x 1 bias vector\n",
    "\n",
    "1. $z_1 = W_1x +b_1$ V x 1, N x V, N x 1\n",
    "2. $ h = ReLu(z_1) $ N x 1\n",
    "3. $ z_2 = W_2h +b_2 $ V x 1, V x N, V x 1\n",
    "4. $ y = softmax(z_2) $ V x 1\n",
    "\n",
    "When dealing with batches, we can stack examples in a batch as a column. so each column of the resulting matrix $ \\hat{y} $ is the prediction a each column of x which corresponds to the context words. \n",
    "\n",
    "ReLu (Rectified linear unit)\n",
    "- $ ReLu(x) = max(0,x)$ transform a value x into positive integers\n",
    "- $ softmax(x) = \\frac {e^{x_i}}{\\sum_{j=1}^V e^{x_j}}$ , $ x \\in \\mathbb{R}^k $, $ softmax(x) \\in [0,1]^k$\n",
    "the result from a softmax activation function can be intrepreted as probabilites. in our context, it is the probabilities of each word being the center word. \n",
    "\n",
    "Cost Funtion \n",
    "$$ J = -\\sum_{k=1}^v y_k log(\\hat{y_k})$$\n",
    "- cross-entropy loss function (common cost function used for classification problem)\n",
    "- prediction (x-axis) to cross-entropy loss (y-axis) will show a decay function (wrong prediction receives penality: high j value while correction prediction receives rewards: low j value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916a6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f19b4c8d",
   "metadata": {},
   "source": [
    "N size of the word embeddigs\n",
    "V size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb500394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.995732273553991"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "325107ac48b2da5046120ef209c99f8354112f87220bf38b855978ba6df01ef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
