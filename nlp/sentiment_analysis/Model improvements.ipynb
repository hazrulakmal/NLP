{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Model Improvements through Large Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hazrul Akmal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_metric, Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import ml_collections\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer,TFAutoModel, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     3130\n",
       "positive    1852\n",
       "negative     860\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_news = pd.read_csv(\"data/fiqa_phrasebank.csv\")\n",
    "financial_news[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...          1\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...         -1\n",
       "2  For the last quarter of 2010 , Componenta 's n...          1\n",
       "3  According to the Finnish-Russian Chamber of Co...          0\n",
       "4  The Swedish buyout firm has sold its remaining...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n",
    "financial_news.Sentiment = financial_news.Sentiment.map(dicto)\n",
    "financial_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    2,    5,    8,   10,   12,   15,   16,   21,   25,\n",
       "            ...\n",
       "            5810, 5814, 5815, 5816, 5818, 5819, 5824, 5825, 5836, 5841],\n",
       "           dtype='int64', length=1852)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_news[financial_news[\"Sentiment\"]==1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp = 1658884281.666415\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "timestamp = datetime.timestamp(now)\n",
    "print(\"timestamp =\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_news.loc[1,\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1997901, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>source</th>\n",
       "      <th>section</th>\n",
       "      <th>language</th>\n",
       "      <th>published_date_clean</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crossing the border for greater opportunities</td>\n",
       "      <td>1970-01-01T08:00:00+08:00</td>\n",
       "      <td>South China Morning Post</td>\n",
       "      <td>business</td>\n",
       "      <td>english</td>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getting rid of bad blood</td>\n",
       "      <td>2008-09-12T16:00:00+00:00</td>\n",
       "      <td>The National</td>\n",
       "      <td>world</td>\n",
       "      <td>english</td>\n",
       "      <td>2008-09-12</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tram a better alternative for Penang</td>\n",
       "      <td>2009-01-16T12:03:58+08:00</td>\n",
       "      <td>The Edge Markets</td>\n",
       "      <td>business</td>\n",
       "      <td>english</td>\n",
       "      <td>2009-01-16</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The new face of Australian wealth</td>\n",
       "      <td>2009-01-20T18:30:00+08:00</td>\n",
       "      <td>The Edge Markets</td>\n",
       "      <td>business</td>\n",
       "      <td>english</td>\n",
       "      <td>2009-01-20</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fraud leads to cut in dental insurance</td>\n",
       "      <td>2009-02-14T16:00:00+00:00</td>\n",
       "      <td>The National</td>\n",
       "      <td>world</td>\n",
       "      <td>english</td>\n",
       "      <td>2009-02-14</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title             published_date  \\\n",
       "0  Crossing the border for greater opportunities  1970-01-01T08:00:00+08:00   \n",
       "1                       Getting rid of bad blood  2008-09-12T16:00:00+00:00   \n",
       "2           Tram a better alternative for Penang  2009-01-16T12:03:58+08:00   \n",
       "3              The new face of Australian wealth  2009-01-20T18:30:00+08:00   \n",
       "4         Fraud leads to cut in dental insurance  2009-02-14T16:00:00+00:00   \n",
       "\n",
       "                     source   section language published_date_clean  year  \n",
       "0  South China Morning Post  business  english           1970-01-01  1970  \n",
       "1              The National     world  english           2008-09-12  2008  \n",
       "2          The Edge Markets  business  english           2009-01-16  2009  \n",
       "3          The Edge Markets  business  english           2009-01-20  2009  \n",
       "4              The National     world  english           2009-02-14  2009  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_news = pd.read_csv(\"data/english_news.csv\")\n",
    "print(english_news.shape)\n",
    "english_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '',text) \n",
    "    text  = re.sub(r'<.*?>' ,'', text)  \n",
    "    text = re.sub(r'\\x89\\S+' , ' ', text) #Removes string starting from \\x89\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  # Removes numbers\n",
    "    text = re.sub(r'[^\\w\\s]','',text)   # Removes Punctuations\n",
    "    return text\n",
    "\n",
    "class config:\n",
    "    PATH = \"../input/nlp-getting-started/\"\n",
    "    MAX_LEN = 36\n",
    "    LOWER_CASE = True\n",
    "    RANDOM_STATE = 12\n",
    "    TEST_SIZE = 0.2\n",
    "    VALIDATION_SIZE = 0.1\n",
    "    NUM_LABELS = 1\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 5e-5\n",
    "    EPOCHS = 10\n",
    "    WEIGTH_DECAY = 0.01\n",
    "    DEVICE = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe, dictionary=False):\n",
    "    \"\"\" split pandas dataframe into train set & test set and stored them in dictionary\n",
    "    Params:\n",
    "        dataframe (Pandas DataFrame) : \n",
    "        \n",
    "    Returns:\n",
    "        dictionary : keys (train, validation, test), values (the sets)\n",
    "\n",
    "    \"\"\"\n",
    "    training_df, test_df = train_test_split(\n",
    "        dataframe,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "    )\n",
    "    if dictionary:\n",
    "        dataset = {\n",
    "            \"train\": Dataset.from_pandas(training_df),\n",
    "            \"test\": Dataset.from_pandas(test_df),\n",
    "        }\n",
    "\n",
    "        dataset = DatasetDict(dataset)\n",
    "        return dataset\n",
    "    else: \n",
    "        return training_df, test_df\n",
    "\n",
    "train, test = create_dataset(financial_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_1 = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_1 , do_lower_case = config.LOWER_CASE , max_length = config.MAX_LEN )\n",
    "x_train = tokenizer(\n",
    "        text = train[\"Sentence\"].to_list(),\n",
    "        add_special_tokens = True,\n",
    "        max_length = config.MAX_LEN,\n",
    "        truncation = True,\n",
    "        padding = True,\n",
    "        return_tensors = \"tf\",\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "        )\n",
    "\n",
    "x_test = tokenizer(\n",
    "        text = test[\"Sentence\"].to_list(),\n",
    "        add_special_tokens = True,\n",
    "        max_length = config.MAX_LEN,\n",
    "        truncation = True,\n",
    "        padding = True,\n",
    "        return_tensors = \"tf\",\n",
    "        return_token_type_ids = False,\n",
    "        return_attention_mask = True,\n",
    "        verbose = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_based_uncased = TFAutoModel.from_pretrained(MODEL_1)\n",
    "input_ids = tf.keras.layers.Input(shape = (config.MAX_LEN,) , dtype = tf.int32 , name = \"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape = (config.MAX_LEN,) , dtype = tf.int32 , name = \"attention_mask\")\n",
    "embeddings = bert_based_uncased(input_ids , attention_mask = input_mask)[1]\n",
    "x = tf.keras.layers.Dropout(0.3)(embeddings)\n",
    "x = tf.keras.layers.Dense(128 , activation = \"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(32 , activation = \"relu\")(x)\n",
    "output = tf.keras.layers.Dense(config.NUM_LABELS , activation = \"sigmoid\")(x)\n",
    "\n",
    "model_1 = tf.keras.Model(inputs = [input_ids , input_mask] , outputs = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  os.path.isdir(\"./weights/bert_base_uncased_weights\") is None:\n",
    "          os.makedirs(\"./weights/bert_base_uncased_weights\")\n",
    "checkpoint_filepath_bert_base_uncased  = \"./weights/bert_base_uncased_weights\"\n",
    "checkpoint_callback_bert_base_uncased = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath_bert_base_uncased,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='auto',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hazrul Akmal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), \n",
    "             optimizer = tf.keras.optimizers.Adam(lr = config.LEARNING_RATE , epsilon = 1e-8 , decay  =config.WEIGTH_DECAY , clipnorm = 1.0),\n",
    "             metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hazrul Akmal\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "bert_based_uncased_history  = model_1.fit(x = {\"input_ids\": x_train[\"input_ids\"] , \"attention_mask\" : x_train[\"attention_mask\"]},\n",
    "                y = train[\"Sentiment\"] , \n",
    "                epochs = config.EPOCHS , \n",
    "                validation_split = 0.2,\n",
    "                batch_size = 256 , callbacks = [checkpoint_callback_bert_base_uncased])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights(checkpoint_filepath_bert_base_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_based_uncased_hist_df = pd.DataFrame(bert_based_uncased_history.history , columns = ['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(bert_based_uncased_hist_df, y=[\"accuracy\" , \"val_accuracy\"], title=\"Accuracy\") \n",
    "fig.update_xaxes(title=\"Epochs\")\n",
    "fig.update_yaxes(title = \"Accuracy\")\n",
    "fig.update_layout(showlegend = True,\n",
    "        title = {\n",
    "            'text': \"Bert Base uncased Accuracy\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"data/fiqa_phrasebank.csv\",\n",
    "        \"model_path\": \"/kaggle/working/bert_model.h5\",\n",
    "        \"model_type\": \"transformer\",\n",
    "\n",
    "        \"test_size\": 0.1,\n",
    "        \"validation_size\":0.2,\n",
    "        \"train_batch_size\": 32,\n",
    "        \"eval_batch_size\": 32,\n",
    "\n",
    "        \"epochs\": 5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"lr\": 3e-5,\n",
    "        \"num_warmup_steps\": 10,\n",
    "\n",
    "        \"max_length\": 128,\n",
    "        \"random_seed\": 42,\n",
    "        \"num_labels\": 3,\n",
    "        \"model_checkpoint\":\"roberta-base\",\n",
    "    }\n",
    "\n",
    "    cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "    return cfg\n",
    "\n",
    "cfg = model_config()\n",
    "\n",
    "def clean_text(df,field):\n",
    "    df[field] = df[field].str.replace(r\"http\\S+\",\" \") #Removes Websites\n",
    "    df[field] = df[field].str.replace(r\"http\",\" \") #Removes Websites\n",
    "    df[field] = df[field].str.replace(r\"@\",\"at\") \n",
    "    df[field] = df[field].str.replace(\"#[A-Za-z0-9_]+\", ' ')\n",
    "    df[field] = df[field].str.replace(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\",\" \")\n",
    "    df[field] = df[field].str.lower()\n",
    "    return df \n",
    "\n",
    "def preprocess_csv(csv_file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    labelencoder = LabelEncoder()\n",
    "    df[\"label_enc\"] = labelencoder.fit_transform(df[\"Sentiment\"])\n",
    "    df.rename(columns={\"label\": \"label_desc\"}, inplace=True)\n",
    "    df.rename(columns={\"label_enc\": \"labels\"}, inplace=True)\n",
    "    df.drop_duplicates(subset=['Sentence'],keep='first',inplace=True) #drop duplicates\n",
    "\n",
    "    cleaned_df = clean_text(df, \"Sentence\")\n",
    "    return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 420M/420M [03:21<00:00, 2.18MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.5358694195747375}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = english_news.iloc[:100,:]\n",
    "headlines = list(map(clean_dataset, small_df[\"title\"].to_list()))\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "classifier(\"I love my day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert = pipeline(\"sentiment-analysis\", model = \"distilbert-base-uncased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "325107ac48b2da5046120ef209c99f8354112f87220bf38b855978ba6df01ef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
