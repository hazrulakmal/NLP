{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTy4exWrCVOVXFSdj4KU8w"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic concepts and PyTorch intoduction"
      ],
      "metadata": {
        "id": "5BYAG57gXCDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "x = torch.tensor(3)\n",
        "y = torch.tensor(4)\n",
        "\n",
        "print(f\"scalar operation: {x+y, x-y}\")\n",
        "\n",
        "x = torch.arange(12, dtype=torch.float32)\n",
        "print(f\"vector: {x}\")\n",
        "print(f\"matrix: {x.reshape(-1, 2)}\")\n",
        "print(f\"matrix dimensions: {x.reshape(-1, 2).shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iD1HaP2W_n1",
        "outputId": "175c838d-9bb4-4e68-a4af-715c79442047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scalar operation: (tensor(7), tensor(-1))\n",
            "vector: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n",
            "matrix: tensor([[ 0.,  1.],\n",
            "        [ 2.,  3.],\n",
            "        [ 4.,  5.],\n",
            "        [ 6.,  7.],\n",
            "        [ 8.,  9.],\n",
            "        [10., 11.]])\n",
            "matrix dimensions: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Algebra "
      ],
      "metadata": {
        "id": "6EEEt9dhXBMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrxbeLdbXYIK",
        "outputId": "87669571-0be8-4ec3-df41-16f7bddf2ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]), tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "A = torch.arange(6, dtype=torch.float32).reshape(-1, 3)\n",
        "B = A.clone() #doing this we allocate a new memory to new variable B\n",
        "A, A+B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# element wise product - Hadamard product \n",
        "\n",
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCRVxVmjaqqn",
        "outputId": "f92f222c-c2f2-4a61-c6e9-71796a307281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  4.],\n",
              "        [ 9., 16., 25.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scalar and tensor elemenst wiseproduct\n",
        "a = torch.tensor(2)\n",
        "a * A, a + A, (a + A).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIJ6h-qzbLh-",
        "outputId": "f8f5019f-7e37-4db3-ede1-d204eb8ef157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  2.,  4.],\n",
              "         [ 6.,  8., 10.]]), tensor([[2., 3., 4.],\n",
              "         [5., 6., 7.]]), torch.Size([2, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduction (sum)\n",
        "\n",
        "x = torch.arange(6, dtype= torch.float32)\n",
        "print(x, x.sum())\n",
        "\n",
        "x_matrix =  x.reshape(-1,3) #trick to not allocate a new memory for x\n",
        "print(x_matrix.shape, x_matrix.sum(axis=0), x_matrix.sum(axis=1)) #axis=1 will reduce the column dimension (axis 1) by summing up elements of all the columns.\n",
        "#other way to remember, 1 is not vertical but horizontal sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8h2RcF8bkSG",
        "outputId": "4f646917-acee-486e-fca5-a893694ee840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 2., 3., 4., 5.]) tensor(15.)\n",
            "torch.Size([2, 3]) tensor([3., 5., 7.]) tensor([ 3., 12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"A: {A}, shape: {A.shape}\")\n",
        "print(f\"element wise mean: {A.mean(), A.sum() / A.numel()}\")\n",
        "print(f\"columwise mean: {A.mean(axis=1), A.sum(axis=1)/A.shape[1], A.mean(axis=1).shape}\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FardqAnvb86g",
        "outputId": "d5fcf537-115f-4efe-c3d4-e8333d909a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: tensor([[0., 1., 2.],\n",
            "        [3., 4., 5.]]), shape: torch.Size([2, 3])\n",
            "element wise mean: (tensor(2.5000), tensor(2.5000))\n",
            "columwise mean: (tensor([1., 4.]), tensor([1., 4.]), torch.Size([2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# non reduction sum - keeping the dims (the number of axes stays the same), This matters when we want to use the broadcast mechanism.\n",
        "\n",
        "sum_A = A.sum(axis=1, keepdims=True)\n",
        "sum_A, sum_A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UW-mVhhc8Rd",
        "outputId": "0546eea6-ac17-4da5-8d1b-533d937a939b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 3.],\n",
              "         [12.]]), torch.Size([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can divide A by sum_A with broadcasting to create a matrix where each row sums up to 1\n",
        "A / sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3bK5CGtfEXM",
        "outputId": "f2053ad8-d7e0-4fcc-cd40-dd8278df9b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.3333, 0.6667],\n",
              "        [0.2500, 0.3333, 0.4167]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row)\n",
        "A, A.cumsum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0-4jINXg0BP",
        "outputId": "db824047-6475-43a1-c759-4610a26a139d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 1., 2.],\n",
              "         [3., 4., 5.]]), tensor([[0., 1., 2.],\n",
              "         [3., 5., 7.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product\n",
        "\n",
        "Dot Product\n",
        "- given two vectors $x, y \\in \\mathbb{R}^d$, the dot product is $x^{\\top}y$\n",
        "- Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector  and a set of weights denoted by the weighted sum of the values in according to the weights could be expressed as the dot product . When the weights are non-negative and sum to one, i.e., the dot product expresses a weighted average. After normalizing two vectors to have unit length, the dot products express the cosine of the angle between them."
      ],
      "metadata": {
        "id": "M3Gi9XxCi_up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dot product \n",
        "x = torch.arange(3, dtype=torch.float32)\n",
        "y = torch.ones_like(x)\n",
        "\n",
        "x, y, torch.dot(x, y), torch.sum(x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ok-UmLFg-b4",
        "outputId": "35426b4e-1142-4d16-d2ec-53cc43cc0c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.), tensor(3.))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix-Vector Products\n",
        "\n",
        "print(f\"matrix-vector: {A.shape, x.shape, torch.mv(A, x), A@x}\")\n",
        "\n",
        "# Matrix-Matrix Multiplication\n",
        "B = torch.ones(3, 4)\n",
        "print(f\"matrix-matrix: {torch.matmul(A, B)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOnggvIbkagn",
        "outputId": "59617c56-9f59-43da-8dc5-272d05db5068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matrix-vector: (torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))\n",
            "matrix-matrix: tensor([[ 3.,  3.,  3.,  3.],\n",
            "        [12., 12., 12., 12.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Norms\n",
        "\n",
        "- Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is. For instance, the $l_2$ norm measures the (Euclidean) length of a vector.\n",
        "- The norm is also popular and the associated metric is called the Manhattan distance. By definition, the $l_1$ norm sums the absolute values of a vector’s elements"
      ],
      "metadata": {
        "id": "JhT-XGvHmq7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#l2 norm\n",
        "u = torch.tensor([3.0, -4.0])\n",
        "print(f\"l2: {torch.norm(u)}\")\n",
        "\n",
        "#l1 norm\n",
        "print(f\"l1 : {torch.abs(u).sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG1HFanglc5v",
        "outputId": "9305e1de-b635-4229-df6c-6a6acff61e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l2: 5.0\n",
            "l1 : 7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic Differentiation"
      ],
      "metadata": {
        "id": "6s3lz3gPxtZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4.0)\n",
        "x.requires_grad_(True)\n",
        "x.grad\n",
        "\n",
        "y = 2 * torch.dot(x, x) #y = 2x^2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfYlUT3fxrZ1",
        "outputId": "d254665b-ae61-4c66-9515-c3ae05231fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now take the gradient of y with respect to x by calling its backward method. Next, we can access the gradient via x’s grad attribute.\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBjA2tpOyZeJ",
        "outputId": "c46eaa6a-6e11-4263-9da4-d358d672a3f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3bjqd_dy1eT",
        "outputId": "9be29b08-42cb-434d-9941-f1ec82a60e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_() #to reset the gradient\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHdgTBM_zt8b",
        "outputId": "7d119c1d-3bb8-459a-c358-736df7eed113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detaching\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u\n",
        "## you dont want to compute the gradient flow from y object, only compute the gradient in z. derivative of z is u and since u is y = x^2 then its gradient x^2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmvstB4i0gRz",
        "outputId": "35a7f3be-efd8-4e1f-f767-f773d54b5bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u, x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IvqJKcW0joZ",
        "outputId": "df1c6bfe-3b10-4bd0-9f28-d27adb9ab975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 4., 9.]), tensor([0., 1., 2., 3.], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "things to remember\n",
        "\n",
        "- (i) attach gradients to those variables with respect to which we desire derivatives; detaching - u = y.detach()\n",
        "- (ii) record the computation of the target value y.sum() \n",
        "- (iii) execute the backpropagation function; and y.sum().backward()\n",
        "- (iv) access the resulting gradient x.grad()\n",
        "\n",
        "x.grad.zero_() #reseting the gradient object"
      ],
      "metadata": {
        "id": "tbh8mvBy2pZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "\n",
        "1. show that the transpose of the transpose of a matrix is the matrix itself: \n",
        "2. Given two matrices $A, B$ and show that sum and transposition commute: $ A^{\\top} + B ^{\\top} = ( A + B )^{\\top}$"
      ],
      "metadata": {
        "id": "ppkS0Nrcpg8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.normal(0, 0.1, (5, 5))\n",
        "torch.transpose(torch.transpose(A, 0, 1),0 ,1) == A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9nKEim_oX1w",
        "outputId": "89b652ba-0b4d-48c2-9f8d-a38ac5f8d885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.randn(5,5)\n",
        "B_t = torch.transpose(B, 0, 1)\n",
        "A_t = torch.transpose(A, 0, 1)\n",
        "\n",
        "A_t + B_t == torch.transpose(A + B, 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yat22iEEqWnx",
        "outputId": "96931dfb-ffda-4db4-d482-868664835ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True],\n",
              "        [True, True, True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OL5JaLcYrQ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjfSRelAyWIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}