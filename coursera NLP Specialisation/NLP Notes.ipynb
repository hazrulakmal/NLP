{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6922574a",
   "metadata": {},
   "source": [
    "Steps Required to model logistics regressions.\n",
    "\n",
    "* Extract useful information from the data and transform them into a set of inputs aka **features**\n",
    "* Train classify model while minimising the cost funtion\n",
    "* Make Prediction \n",
    "\n",
    "### Feature Extraction (Fequency Dictionary Mapping)\n",
    "Represent a text in  a vector of dimension |v| (Vocabulary size)\n",
    "* features are a list of words (vocabulary)\n",
    "* numerical representation. if a word exists then the corresponding feature is marked one. if the word appears twice, it is marked two and so on. \n",
    "* number of parameters, n, is equal to number of features, |v|\n",
    "\n",
    "Sparse Representations\n",
    "for a large vocabulary model, two problems arise\n",
    "* expensive computational time\n",
    "* overfiting. Model is complex (too many parameters)\n",
    "\n",
    "Vocabulary frquence vector (dictionary mapping from words to frequency) counts the number times of word appears for in either positive or negative. \n",
    "Feature extraction \n",
    "Encode three terms - bias, positive features & negative features\n",
    "positive - counts the freq of words that appear in positive vocabulary frequency vector\n",
    "negarive - counts the freq of words that appear in negative vocabulary frequecy table\n",
    "\n",
    "### Preprocessing Text\n",
    "1. Eliminate handles and URLs\n",
    "2. Tokenize the strings into words (process by which a large quantity of text is devided into smaller parts - remove duplicates, punctuations)\n",
    "3. remove stops words\n",
    "4. convert all words to lower case\n",
    "5. stemming the words - transform each word to its root words\n",
    "\n",
    "### General NLP Steps\n",
    "1. Perform preprocessing \n",
    "2. Feature extraction to convert text into numerical representation. Dictionary Frequecy Mapping (list all words in positive text and negative text sepreately). for each tweet, Extract 3 columns (bias, sum of positive words, sum of negative words). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c87536",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Logistics Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad491",
   "metadata": {},
   "source": [
    "refer to week 1 notebook for codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321e471",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Classifier with Naive Bayes Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725dd31",
   "metadata": {},
   "source": [
    "Probability is a fundamental concept in NLP tasks. \n",
    "\n",
    "### Naive Bayes Inference Conditional Rule for Binary Classification \n",
    "For a balance dataset, product of all ratios of the probability of each word given positive class and probability of similar word given the negative class. if the value is bigger than one, the numerator probiblity > than the denominator. we classify the text as positve. \n",
    "\n",
    "$$ \\prod_{i=1}^m \\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)}$$\n",
    "\n",
    "non-balance dataset requires a prior distribution $ \\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "The problem with this approach is that some words may appear in one class but not the other. this means that the probability for the class without the word is zero and when the above approach is computed, we get a calculated value of zero - not good loss of information. So to combat this problem we use laplacian smoothing. \n",
    "\n",
    "$$ P(W^{(i)} | class ) = \\frac {freq (w^{(i)} \\cap class) + 1}{N_{class} + V_{class}} $$\n",
    "\n",
    "- $ V_{class} $ refers to number of unique words in vocabulary\n",
    "- $ N_{class} $ refers to frequency of all words in class\n",
    "\n",
    "\n",
    "As $ m $ gets larger, we will encourter numerical overflow issues - that is a problem when the number is very small for computer to store. We use the propreties of log transformation to solve this issues. \n",
    "\n",
    "To make an inference, we now use this formula\n",
    "\n",
    "$$ \\sum_{i=1}^m \\lambda(w^{(i)}) = \\sum_{i=1}^m log\\frac{P(W^{(i)}|pos)}{P(W^{(i)}|neg)} $$\n",
    "\n",
    "A small ratio less than one produces negative value while ratio bigger than one produces positive value. The bigger/smaller the ratio, the bigger/smaller the log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174c959",
   "metadata": {},
   "source": [
    "#### Train Naive Bayes\n",
    "\n",
    "1. Collect and annote corpus \n",
    "2. Preprocessing (lowercase, remove punctuations, url, names, remove stop words, stemming word, tokenize sentences) from raw data (messy) into a clean data. (Take a big chuck of the whole project)\n",
    "3. Compute word count $ freq(W, class) $ to produce freq table\n",
    "4. Get conditional probility of a word given the class $ P(w^{(i)}|pos) $ $ P(w^{(i)}|neg) $ using laplace smoothing\n",
    "5. Get the lambda, $ \\lambda(w^{(i)}) $ \n",
    "6. Compute logprior $ log\\frac{P(pos)}{P(neg)} $\n",
    "\n",
    "\n",
    "### Application of Naive Bayes Model\n",
    "- Sentiment Analysis\n",
    "- Author Auntentication - given  a text, predict from which author the text is written by\n",
    "- Spam Classification\n",
    "- Information Retirieval - relevant vs irrelavant document based on keyword input\n",
    "\n",
    "\n",
    "### Assumptions it holds\n",
    "1. Independece - not true in NLP. some words appears in pair more often than the other\n",
    "2. Relative frequence of class in corpus affects the model - more positive class then the model is biased towards this class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b715c",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "represent words and documents as vectors to capture the relative meanings of words in a sentence. \n",
    "some applications in machine translations, chatbox and text extraction. \n",
    "\n",
    "Two design paradigms \n",
    "- Word by word design - fix bandwith $k$ , distance for which to decide wether two words are next to one another \n",
    "- Word by Document design - keep tracks of number of times a word appears in a document in matrix form. words stored in row\n",
    "\n",
    "\n",
    "#### Measure of similiarity \n",
    "- euclidean distance on n-dimensional space $ d(\\vec a, \\vec b) = \\sqrt {\\sum_{i=1}^{n}(\\vec a_{i}- \\vec b_{i})^2} = || \\vec a - \\vec b || = norm(\\vec a - \\vec b) $\n",
    "- cosine similiarity can be a better proxy for similiarity than eucliden distance one vector is shorter than the other but they are close to one another as it is not biased on the size of corpus representations (vector length). Here the angle between them better captures the clossness between the two vectors. Recall $ \\vec a \\cdot \\vec b = \\sum_{i=1}^n ( a_{i} \\cdot  b_{i})$ so cosine metric is given by $ cos(\\beta) = \\frac {\\vec a \\cdot \\vec b}{||\\vec a|| \\cdot ||\\vec b||}$\n",
    "- two orthogonal vectors = Maximally dissimilar \n",
    "- notice that $ -1 \\leq cos(\\beta) \\leq 1 $, the higher the cosine value, the smaller the angle. Hence the closer the two vectors are. \n",
    "- if  $ cos(\\beta)$ is between -1 and 0 then the two vectors are dissmiliar. (Recall the cartesian plane cosine rule - cosine is only positive on first and fourth quardrant.)\n",
    "- if two vectors are identical then $ cos(\\beta) = 1$ \n",
    "\n",
    "We use measure of similiarity to predict closest meaning word for a given a word. The catch here is to represent the vector space where the word representations capture the relative meaning of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f33eb",
   "metadata": {},
   "source": [
    "#### Predicting rela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333d0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n",
      "6.4031242374328485\n",
      "12.083045973594572\n",
      "0.08512565307587484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "turkey = [3,1]\n",
    "ankara = [9,1]\n",
    "japan = [4,3]\n",
    "usa = [5,6]\n",
    "wash = [10, 5]\n",
    "\n",
    "v1 = [1,2,3]\n",
    "v2 = [4,7,2]\n",
    "v3 = [3,1,4]\n",
    "v4 = [1,0,-1]\n",
    "v5 = [2,8,1]\n",
    "\n",
    "def euclidean(array1, array2):\n",
    "    return np.linalg.norm(np.array(array2)-np.array(array1))\n",
    "\n",
    "def cosine(array1, array2):\n",
    "    array1 = np.array(array1)\n",
    "    array2 = np.array(array2)\n",
    "    return (np.dot(array1, array2))/(np.linalg.norm(array1)*np.linalg.norm(array2))\n",
    "\n",
    "city = np.array(usa) - np.array(wash)\n",
    "country = np.array(ankara) + city\n",
    "print(euclidean(japan, country))\n",
    "print(euclidean(turkey, country))\n",
    "\n",
    "distance = np.array(v1) - np.array(v2)\n",
    "desired = distance + np.array(v3)\n",
    "print(euclidean(v1, desired))\n",
    "print(euclidean(v2, desired))\n",
    "print(cosine(v4,v5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6eed7",
   "metadata": {},
   "source": [
    "### PCA \n",
    "Dimensionality reduction technique that projects n dimensional space to a smaller dimension. \n",
    "\n",
    "Application\n",
    "- reduce n dimensional space into two dimension and plot them on a 2-D graph to see where the data points are relative to others. \n",
    "\n",
    "How it works?\n",
    "1. Find the eigenvector and eigenvalues of the matrix\n",
    "2. Eigenvector gives the direction of uncorrelated features\n",
    "3. Eigenvalues : amount of information retained by new features which tells how much variance there is in the vector. \n",
    "\n",
    "Steps\n",
    "1. mean normalize the data\n",
    "2. find the covariance matrix\n",
    "3. find the eigenvalues and eigenvectos of the covariance matrix\n",
    "4. rearrange the eigenvectors such that its eigenvalues are in deacreasing order. \n",
    "5. take a subset of the first n eigenvectors and multiply them with the normalize data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d969875",
   "metadata": {},
   "source": [
    "## Machine Translation\n",
    "\n",
    "1. Word embeddings in English (X) and Word embeddings in France (Y)\n",
    "2. To find the relationshipe between vector space X and Y, we need to find the maxtrix R, where XR $ \\approx $ Y\n",
    "\n",
    "Finding R using  frobenius norm\n",
    "1. initiate loop Loss = $|| XR - Y ||_F$\n",
    "2. Minimise Loss by taking its derivative $ g = \\frac{d}{dR}$Loss\n",
    "3. Update R = R - $\\alpha \\times g$ where $ \\alpha $ is a learning rate\n",
    "4. Stop when Loss falls within a certain threshold\n",
    "\n",
    "#### Frobenius norm, \n",
    "$A$ is m x n matrix\n",
    "$$ ||A||_F = \\sqrt {\\sum^m_{i=1} \\sum^n_{j=1} |\\alpha_{ij}|^2}$$\n",
    "\n",
    "#### Hast Table & Hash Function\n",
    "\n",
    "A data structure used to query a data from a table that runs constant time $O(1)$ is faster than a linear search $O(n)$\n",
    "Hash Function is a function that gives a hash value (key).\n",
    "\n",
    "A simple hash table that stores a list of number in n buckets is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d13eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.14142842854285\n",
      "7.14142842854285\n"
     ]
    }
   ],
   "source": [
    "#frobenium norm \n",
    "\n",
    "v = np.array([[1,3], [4,5]])\n",
    "\n",
    "print(np.sqrt(np.sum(np.square(v))))\n",
    "print(np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123be5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [1],\n",
       " 2: [22, 12],\n",
       " 3: [3, 13],\n",
       " 4: [44],\n",
       " 5: [],\n",
       " 6: [],\n",
       " 7: [77],\n",
       " 8: [88],\n",
       " 9: [9]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_function(value, n_buckets):\n",
    "    return value % n_buckets\n",
    "\n",
    "def basic_hash_table(values, n_buckets):\n",
    "    \"\"\" values : a list of elements \n",
    "        n_buckets : a scalar\n",
    "    \"\"\"\n",
    "    hash_table = {i:[] for i in range(n_buckets)}\n",
    "    for value in values:\n",
    "        hash_value = hash_function(value, n_buckets)\n",
    "        hash_table[hash_value].append(value)\n",
    "    return hash_table\n",
    "\n",
    "list_num = [1,44,22,77,3,88,9,13,12]\n",
    "\n",
    "table = basic_hash_table(list_num, 10)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f554088",
   "metadata": {},
   "source": [
    "#### Locality sensitive hashing\n",
    "used to reduce **the computational cost** of finding k-neareast neighbour in high dimensional space. \n",
    "a hash function that takes into account the relative location of a vector in a vector space to build up the hash table. \n",
    "\n",
    "1. Given a plane, find a normal vector perpendicular to the given plane.\n",
    "2. Find the dot product of a vector representing the data (need to be transposed) and the normal vector\n",
    "3. Extract the resulting vector sign to decide wether the data is below or above the plane (location of a data point relative to a plane). \n",
    "4. Generalise this idea into multiple planes \n",
    "\n",
    "#### Multiple plane generalisation\n",
    "given a point denoted by $v$, we can run in on several planes, $P_1,P_2,P_3$. If the sign of $P_1v^T$ is posivte, then we label $h_1$ as 1 while 0 if it is negative. A hash function is created such that \n",
    "$$hashvalue = \\sum_{i=0}^H 2^i \\times h_{i+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a9d70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def side_of_planes(P, v):\n",
    "    \"\"\"\n",
    "        P, V both need to be a vector\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(P, v.T)\n",
    "    sign = np.sign(dot_product)\n",
    "    sign_scalar = np.asscalar(sign)\n",
    "    return sign_scalar\n",
    "\n",
    "def hash_multiple_plane(Ps, v):\n",
    "    \"\"\" Ps : a list of multiple plane\n",
    "        v : vector representing a data point\n",
    "    \"\"\"\n",
    "    hash_value = 0\n",
    "    for i, P in enumerate(Ps):\n",
    "        h_sign = side_of_planes(P, v)\n",
    "        h_i = 1 if h_sign >= 0 else 0\n",
    "        hash_value += 2**i * h_i\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67724f42",
   "metadata": {},
   "source": [
    "#### Approximate Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e05aff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30431584,  0.4105472 ,  0.71611908,  0.49633876],\n",
       "       [ 1.22778007,  1.55515613, -0.06870154, -0.67537823],\n",
       "       [-2.4299803 , -1.07048808,  0.41357607,  1.04544117]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dimensions = 2\n",
    "num_planes =3 \n",
    "\n",
    "random_plane_matrix = np.random.normal(size =(num_planes, n_dimensions))\n",
    "\n",
    "def side_of_planes(Ps, v):\n",
    "    dotprod = np.dot(Ps, v.T)\n",
    "    sign = np.sign(dotprod)\n",
    "    return sign\n",
    "\n",
    "v = np.array([[1,2]])\n",
    "print(v.shape)\n",
    "side_of_planes(random_plane_matrix, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d604c",
   "metadata": {},
   "source": [
    "1. Auto Correct\n",
    "    - n-edit distance\n",
    "    - minimum edit distance (dynamic programming)\n",
    "    - edits (replace, insert, switch, delete)\n",
    "\n",
    "2. Part of speech Tagging\n",
    "    - Hidden Markov Chains\n",
    "    - Populating transition matrix & emission matrix\n",
    "    - Vertibi algorithm (initialisation, forward pass & backward pass) to find the highest probability POS tagging in a sentence\n",
    "\n",
    "3. N-grams autocomplete language model\n",
    "    - N-Grams and probabilities\n",
    "    - Approximate sentence probability from N-Grams\n",
    "    - Build a language model from a corpus\n",
    "    - Fix missing information\n",
    "    - Out of vocabulary words with <UNK>\n",
    "    - Missing N-Gram in corpus with smoothing, backoff and interpolation\n",
    "    - Evaluate language model with perplexity\n",
    "\n",
    "4. Words embedding\n",
    "    - Identify the key concepts of word representations\n",
    "    - Generate word embeddings\n",
    "    - Prepare text for machine learning\n",
    "    - Implement the continuous bag-of-words model\n",
    "\n",
    "## Autocorrect and Minimum Edit Distance\n",
    "\n",
    "A genenral step to do autocorrect \n",
    "1. identify misspelled word - can check wether the word is in dictionary. there's more sophisicated technique that identifies missplled word such as looking at the word surrounding it to understand the context of a sentence. \n",
    "2. comupte string n edit distance away. type of edits are insert, delete, swap and replace. \n",
    "3. filter candidates (only include real words)\n",
    "4. find word probabibilities (choose a word that is the most likely to occur in the context) number of words occuring in corpus devided by the total number of words in the corpus. \n",
    "\n",
    "## Part of speech tagging\n",
    "\n",
    "tag each word with its category. ex something(noun) play (verb) why(adverb)\n",
    "\n",
    "this technique is suetable for tasks such as identifying named entities, speech recognition \n",
    "\n",
    "\n",
    "## Markov Chain\n",
    "type of stochastic model that describes a sequence of possible events. to get the probability of each event, they only need probability of the previous event. \n",
    "\n",
    "Stochastic - random\n",
    "\n",
    "Morkov chain consits of n states \n",
    "Transition matrix (transition probilities) that keep tracks of the probabilities of going from one state to another state is n+1 by n matrix.\n",
    "\n",
    "Populate transition matrix\n",
    "1. find the occurances of tag pairs $ C(t_{i-1}, t_i)$\n",
    "2. calculate the probabilities of the count\n",
    "$$ P(t_i|t_{i-1}) = \\frac {C(t_{i-1}, t_i)}{\\sum_{j=1}^n C(t_{i-1}, t_j)} $$\n",
    "\n",
    "number of times $t_i , t_{i-1}$ show up next to each other devided by the total number of times $t_{i-1}$ shows up in the corpus.\n",
    "\n",
    "Transition matrix smoothing $ \\epsilon $ is arbitrarily small value\n",
    "$$ P(t_i|t_{i-1}) = \\frac {C(t_{i-1}, t_i) + \\epsilon}{\\sum_{j=1}^N C(t_{i-1}, t_j) + N \\cdot \\epsilon} $$\n",
    "\n",
    "in the real word, one might not want to add $ \\epsilon $ to the first row of the matrix because that means we are allowing any kind of part of speach to begin a sentence. that's not true for certain POS such as punctuation. smoothing is important to avoid devision by zero in the case where POS does not appear in the corpus and better generalisation. some POS tags may not appear in the corpus but it does possible to happen. \n",
    "\n",
    "## Hiden Markov Chain\n",
    "\n",
    "refers to states that are unobservable(unseen)\n",
    "make use of emission matrix that gives probabilities of going from one state to a spesific word. for example given that you are in verb state, you can go to other word with a certain probibility. the middle state wont be exits so it is a nxn matrix.\n",
    "\n",
    "## Vertibi Algorithm\n",
    "1. identify the word in a sentece\n",
    "2. find the hidden state for the word and identify the path\n",
    "3. choose the path with the highest joint probability (between emission and transition probabilities).\n",
    "4. caculate the sequence path probability by taking the product of all the joint probabilities identified. \n",
    "\n",
    "Let A be Transition matrix, B be Emission matrix, C and D (both nxk matrix) be auxilary matrix where C keeps tracks of the probabilities of the whole sequence and D keeps track of the path in the markov chain, K is number of words in a sentence. \n",
    "\n",
    "1. Initialisation of C and D matrix\n",
    "    -  $ c_{i,1} = a_{1,i} * b_{i,cindex_(w_i)}$  &  $d_{i,1} = 0$\n",
    "2. Forward pass (At each step, you calculate the probability of each path happening and the best paths up to that point.)\n",
    "    -  $ c_{i, j} = \\max_{k}c_{k,j-1} * a_{k,i} * b_{i, cindex_(w_i)}$\n",
    "    -  $ c_{i, j} = \\argmax_{k}c_{k,j-1} * a_{k,i} * b_{i, cindex_(w_i)}$\n",
    "3. Backward pass (This allows you to find the best path with the highest probabilities)\n",
    "    -  $ s =  \\argmax_i c_{i,K} $\n",
    "    -  once s is identified, we can tranverse the path in matrix D in reverse order since each cell indicates the previous state the path is taking. \n",
    "\n",
    "\n",
    "## Autocomplete language model\n",
    "\n",
    "using probabilities to predict the next word given the first n-words. the model can also be used for auto-correct task and a search suggesting tools. \n",
    "\n",
    "Skills developed in this\n",
    "1. process a text corprus to N-gram language model\n",
    "2. Handle out of vocabulary words\n",
    "3. implement smoothing for previously unseen n-grams\n",
    "4. language model evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c92e7",
   "metadata": {},
   "source": [
    "1. Creating a language model\n",
    "2. Dealing with out of vocabulary words\n",
    "3. Improve model (more generalisable) with smoothing. \n",
    "\n",
    "## N-gram and Probabilities\n",
    "A sequence of N words/characters\n",
    "1. Unigrams a single word appears in the corpus\n",
    "2. Bigrams two words appear next to one another in the corpus\n",
    "\n",
    "Note on notations\n",
    "- $w_1^m = w_1 w_2 w_3 \\dots w_m$\n",
    "- $w_{m-2}^m = w_{m-2} w_{m-1} w_{m}$\n",
    "\n",
    "Capture the set of words from the value of the subscript to the value of the superscript. for example \"I love machine learning\", $ w_2^3$ = \"love machine\"\n",
    "\n",
    "N-gram probability \n",
    "$$ P(w_N|w_1^{N-1}) = \\frac{P(w_N, w_1^{N-1})}{P(w_1^{N-1})} =\\frac {c(w_1^{N-1}, w_N)}{c(w_1^{N-1})} = \\frac {c(w_1^N)}{c(w_1^{N-1})}$$\n",
    "\n",
    "problem with the probability calculation above is that as the sentence length get lengthy, the probability value tends to approach arbitraly small value (not good for comupter) so we use Markov assumption which indicates only the last word matters so probability of a sentence can be calculated as, for bigram model \n",
    "$$ P(w_1^N) \\approx \\prod_{i=1}^N P(w_i | w_{i-1})$$\n",
    "\n",
    "we need to adjust the starting and ending of a sentece so that we dont undercount. To generalize to an N-gram language model, you can add N-1 start tokens **\\<s>**. \n",
    "\n",
    "For the end of sentence token **\\</s>**, you only need one even if it is an N-gram. Here is an example: \n",
    "\n",
    "N-gram language model\n",
    "- probability count algorithm that avoids undeflow when multplying a lot of values ranging from 0 to 1\n",
    "\n",
    "1. count matrix rows correspond to unique corpus N-1 grams and columns correspond to unique corpus word\n",
    "2. probility matrix - devide each cell with the sum of all values for each row in the count matrix $sum(row) = c(w_{n-N+1}^{n-1})$ each cell formula is then given by $ \\frac {c(w_{n-N+1}^{n-1}, w_n)}{sum(row)}$\n",
    "3. to avoid numerical underflow (computer not good in storing small number hence prone to error), we use log\n",
    "\n",
    "### Generative language model\n",
    "ex bigram model based on highest probility\n",
    "1. choose sentence start\n",
    "2. choose next bigram starting with previous word\n",
    "3. continue until \\<s> is picked\n",
    "\n",
    "Language model evaluation\n",
    "1. split data into train, validation and test set. small corpus 8:1:1 while large corpus 98:1:1\n",
    "2. persplexity score. good model generally have 20-60 score  ($log_2$ perplexity between 4.3-5.9). it differentiates the text wether it is written by a human or a simple program choosing words at random. \n",
    "\n",
    "- $ W $ = test set containing m sentences s,\n",
    "- $ s_i = $ i-th sentence in the test set, each ending with \\</s>\n",
    "- m = the number of all words in the entire test set w excluding \\<s>\n",
    "\n",
    "General formula\n",
    "$$ PP(W) = P(s_1, s_1, \\dots , s_m)^{\\frac {1}{m}}$$\n",
    "\n",
    "bigram formula\n",
    "$$ PP(W) = \\sqrt[m] { \\prod_{i=1}^m \\frac {1}{p(w_i|w_{i-1})}} \\tag{1}$$\n",
    "equivalently\n",
    "$$ log_2PP(W) = - \\frac {1}{m} \\sum_{i=1}^m log_2((p(w_{i}|w_{i-1}))) \\tag{2}$$\n",
    "\n",
    "### Dealing with unknown word encounter in a test set.\n",
    "In some applications of language model, one may encounter a word that does not appear in the training.this word is refered to as Out-of-vocabulary(OOV). replace every unknown word with \\<unk>. \n",
    "1. create a vocabulary V\n",
    "2. replace any word in corpus and not in V by \\<unk>\n",
    "3. count the probabilities with \\<unk> just like any other words the corpus\n",
    "\n",
    "How to create a vocabluary V?\n",
    "- use the same training corpus but applies certain criteria to filter words worth being vocabluary\n",
    "    1. min word freq f. word appears more than f times are included\n",
    "    2. limit |V| size and include the top |v| word in term of frequency\n",
    "\n",
    "- carefull with the vocabulary assessment. frequent \\<unk> may result in better perplexity score but contains a lot of \\<unk> not preferable\n",
    "- only compare languange models perplexity score with the same V\n",
    "\n",
    "### smoothing\n",
    "$ P(w_N|w_1^{N-1}) = \\frac {c(w_1^{N-1}, w_N)}{c(w_1^{N-1})}$ can be 0  if a count of an n-gram is zero or $w_n$ does not appear in V\n",
    "1. add-one(laplacian)/add-k smoothing\n",
    "    - $\\frac {c(w_1^{N-1}, w_N) + 1}{c(w_1^{N-1}) + N}$ (add-one) works if the corpus is large enough to make adding one is negligable \n",
    "    - $\\frac {c(w_1^{N-1}, w_N) + k}{c(w_1^{N-1}) + K*N}$ (add-k) works if corpus is too big that addinng one to the numerator make no difference to the smoothing at all\n",
    "2. backoff\n",
    "    - Stupid” backoff: If the higher order N-gram probability is missing, the lower order N-gram probability is used, just multiplied by a constant. A constant of about 0.4 was experimentally shown to work well.\n",
    "3. interpolation\n",
    "    - introducing lambda for each conditional probability. the sum of all lamdas must equal to 1. lambda can be derived by optimising the expected total conditional probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca335e",
   "metadata": {},
   "source": [
    "Word embeddings (word vectos)\n",
    "- learn how word vectors are created\n",
    "\n",
    "word embeddings applications\n",
    "- sentiment analysis\n",
    "- customer feedback classification\n",
    "- semantic analogies and similiarity\n",
    "- machine translation \n",
    "- information extraction\n",
    "- question-answering \n",
    "\n",
    "Basic word representations\n",
    "1. integers\n",
    "2. one hot encoding, pros: simple and no implied ordering. cons: sparse matrix requires extra space & encode no meanigful meaning.\n",
    "3. word embeddings\n",
    "\n",
    "How to create word embeddings\n",
    "to create one, we always need a corpus of text and an embedding method.\n",
    "\n",
    "1. the corpus of text used is important to capture the semantic meaning between words. Hence, the context of a text is importantwhen creating word embeddings.\n",
    "2. there are many possible methods to learn word embeddings. A machine learning model performs learning task and the by-product of this task are the word embeddings.\n",
    "\n",
    "When training a word vector, there are parameters to finetune (the dimension of the word vector - the higher the dimension, the more nauche the word vector can capture but at a cost of computational cost)\n",
    "\n",
    "### Word embedding method\n",
    "**Classical Methods**\n",
    "1. word2vec (Google, 2013)\n",
    "    - Continuous bag-of-words (CBOW): the model learns to predict the center word given some context words.\n",
    "    - Continuous skip-gram / Skip-gram with negative sampling (SGNS): the model learns to predict the words surrounding a given input word.\n",
    "2. Global Vectors (GloVe) (Stanford, 2014): factorizes the logarithm of the corpus's word co-occurrence matrix,  similar to the count matrix you’ve used before.\n",
    "3. fastText (Facebook, 2016): based on the skip-gram model and takes into account the structure of words by representing words as an n-gram of characters. It supports out-of-vocabulary (OOV) words.\n",
    "\n",
    "**Deep learning, contextual embeddings**\n",
    " \n",
    " In these more advanced models, words have different embeddings depending on their context. You can download pre-trained embeddings for the following models. \n",
    "\n",
    "1. BERT (Google, 2018):\n",
    "2. ELMo (Allen Institute for AI, 2018)\n",
    "3. GPT-2 (OpenAI, 2018)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659385da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b1dc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i', 'am']\thappy\n",
      "['am', 'happy', 'i', 'am', 'learning']\tbecause\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "#sliding window funtion\n",
    "\n",
    "def sliding_window(words, C):\n",
    "    \"\"\" collect center word and context words in a list\n",
    "    Args:\n",
    "        word (list): a list of tokenized words\n",
    "        C (int): the number of words surrounding the center word (context size)\n",
    "    \"\"\"\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_word = words[(i-C):i] + words[(i+1):(i+C+i)]\n",
    "        yield context_word, center_word #data generator. a function that keeps giving us data in small batches\n",
    "        i += 1\n",
    "\n",
    "for x, y in sliding_window(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77ab8c",
   "metadata": {},
   "source": [
    "## NN architecture \n",
    "\n",
    "input layers -> calculate the weighted average of input values + bias and feed in to an activation function -> hidden layers input\n",
    "V x 1\n",
    "N x V weighting matrix between input and hidden layers\n",
    "N x 1 bias vector\n",
    "\n",
    "1. $z_1 = W_1x +b_1$ V x 1, N x V, N x 1\n",
    "2. $ h = ReLu(z_1) $ N x 1\n",
    "3. $ z_2 = W_2h +b_2 $ V x 1, V x N, V x 1\n",
    "4. $ y = softmax(z_2) $ V x 1\n",
    "\n",
    "When dealing with batches, we can stack examples in a batch as a column. so each column of the resulting matrix $ \\hat{y} $ is the prediction a each column of x which corresponds to the context words. \n",
    "\n",
    "ReLu (Rectified linear unit)\n",
    "- $ ReLu(x) = max(0,x)$ transform a value x into positive integers\n",
    "- $ softmax(x) = \\frac {e^{x_i}}{\\sum_{j=1}^V e^{x_j}}$ , $ x \\in \\mathbb{R}^k $, $ softmax(x) \\in [0,1]^k$\n",
    "the result from a softmax activation function can be intrepreted as probabilites. in our context, it is the probabilities of each word being the center word. \n",
    "\n",
    "Cost Funtion \n",
    "$$ J = -\\sum_{k=1}^v y_k log(\\hat{y_k})$$\n",
    "- cross-entropy loss function (common cost function used for classification problem)\n",
    "- prediction (x-axis) to cross-entropy loss (y-axis) will show a decay function (wrong prediction receives penality: high j value while correction prediction receives rewards: low j value)\n",
    "\n",
    "- N size of the word embeddigs\n",
    "- V size of the vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916a6f3",
   "metadata": {},
   "source": [
    "## Training a CBOW model: Forward Propagation \n",
    "\n",
    "Traning process \n",
    "1. propagate examples (batches) forward to the network\n",
    "2. Calculate the cost/loss\n",
    "3. Backpropagation and gradient descent to optimise the parameters of the model\n",
    "\n",
    "Cost function per batch\n",
    "$$ J_{batch} = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j= 1}^V y_j^{(i)} log \\hat{y}_j^{(i)}$$\n",
    "\n",
    "Backpropagation: calculate partial derivatives of cost with respect to weights and biases\n",
    "gradient decent: update the weights and biases\n",
    "alpha tells the model how much you want to update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0faf4b",
   "metadata": {},
   "source": [
    "## Extracting Word Vectors\n",
    "\n",
    "there are two ways of extracting word embeddings after traing continuous bag of word model. \n",
    "1. use w1\n",
    "2. use w2\n",
    "3. use the average of w1 and w2\n",
    "\n",
    "for w1, each column represents a word embedding for a given word (recall that w1 is a NxV maztrix)\n",
    "for w2, each row represents a word embeeding for a given word (recall that w12 is a VxN  matrix)\n",
    "\n",
    "## Evalutaing Word Embeddings\n",
    "**Intrinsic evaluation** \n",
    "allows one to test relationships between words. capture the semantic and syntactic  meaning between words\n",
    "semantic is like japan relates to tokyo, france relates to ?\n",
    "syntactic is like seen is to saw, been is to ? (word similarity based on gramatical rules)\n",
    "\n",
    "Several ways of doing this \n",
    "- anologies\n",
    "- clusteing\n",
    "- visualisation (PCA) based on human judgement wether the words are closely located to one another. \n",
    "\n",
    "**Extrinsic evalutation**\n",
    "evaluate word embeddings on external tasks like name-entity recognition, POS tagging etc\n",
    "\n",
    "this approach is useful to evalute word embedding on the actual task we want to perform but the drawbacks are \n",
    "- unable to identify which process of the word embedding is responsible for the poor performance \n",
    "- time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a1a88",
   "metadata": {},
   "source": [
    "# Neural Network for Sentiment Analysis\n",
    "\n",
    "forward propagation notation\n",
    "\n",
    "$ \\alpha^{[i]} $ represents ith activation layers\n",
    "\n",
    "$$ \\alpha^{[0]} = X $$\n",
    "$$ z^{[i]} = W^{[i]}  \\alpha^{[i-1]} $$\n",
    "$$ \\alpha^{[i]} = g^{[i]}(z^{[i]})$$\n",
    "\n",
    "\n",
    "represents a sentence by converting each word into a number. Since sentences have variable length and input has to be a fixed vector, we use fadding on shorter sentece by adding 0 to its vector to match the longest sentence vector. \n",
    "\n",
    "\n",
    "Why Trax?\n",
    "- Deep learning library that focuses on clear code and speed \n",
    "\n",
    "TensorFlow was built for speed and easy to program. \n",
    "\n",
    "Each layer consists of two components, dense and activation. Dense takes in a parameter value that distates the number of hidden unit in a single layer. \n",
    "other layers are embedding and mean layers. embedding layers are trainable that can learn word embeddings for each word in your vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b4c8d",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "Drawbacks of using N-grams model for language modelling (traditional method)\n",
    "- quick recap of n-gram langugeage model ~ use conditional probability to calculate a propability of a given sentece\n",
    "- inefficient approach to capture dependecies of words in a long sentence.\n",
    "- requires large ram and memory to store probability of sentence with various length\n",
    "- limit to only N prior words which may not capture the entire meaning of a sentence.\n",
    "\n",
    "Advantages of RNN\n",
    "- propagate the meaning of the word from the begining of a sentence until the end. \n",
    "- compute value over and over again to themselves. as the rnn get longer, the earlier information get smaller. \n",
    "- Note that as we feed in more information into the RNN model, the previous word's retention gets weaker, but it is still there. \n",
    "\n",
    "RNN Architectures (input to output)\n",
    "- one to one\n",
    "- one to many (take a photo and generate a sentence)\n",
    "- Many to one (sentiment analysis)\n",
    "- Many to Many (machine translation)\n",
    "\n",
    "Hidden state allows information to propagate through time/different position within a sequence\n",
    "\n",
    "\n",
    "RNN Cost Function \n",
    "- simply summing over all the time steps and dividing by T, to get the average cost in each time step. Hence, we are just taking an average through time.\n",
    "\n",
    "Vanilla RNN, for a long sequence of words, the information tends to vanish. it suffers from vanishing gradient problem\n",
    "\n",
    "RNNS Computation\n",
    "\n",
    "$ h^{[t]} $ t-th hidden state\n",
    "\n",
    "$$ h^{[t]} = g(W_h[h^{[t-1]}m  x^{[t]}]) + b_h $$\n",
    "$$ \\hat{y}^{[t]} = g([W_{yh} h^{[t]}, x^{[t]}]) + b_y $$\n",
    "GRUs Computation\n",
    "\n",
    "$ h^{'[t]} $ temporary t-th hidden state\n",
    "\n",
    "$$ \\Gamma_u = \\sigma(W_u[h^{[t-1]}, x^{[t]}]) + b_u $$\n",
    "$$ \\Gamma_r = \\sigma(W_r[h^{[t-1]}, x^{[t]}]) + b_r $$\n",
    "\n",
    "$$ h^{'[t]} = tanh(W_h[ \\Gamma_r * h^{[t-1]}, x^{[t]}]) + b_h $$\n",
    "$$ h^{t} = (1-\\Gamma_u)*h^{[t-1]} + \\Gamma_u* h^{'[t]} $$\n",
    "$$ \\hat{y}^{[t]} = g([W_h h^{[t]}, x^{[t]}]) + b_h $$\n",
    "\n",
    "\n",
    "- The first gate  $\\Gamma_{u} $ allows you to decide how much you want to update the weights by. \n",
    "- The second gate  $\\Gamma_{r}$ helps you find a relevance score. You can compute the new hh by using the relevance gate. Finally you can compute h, using the update gate. GRUs “decide” how to update the hidden state. GRUs help preserve important information.\n",
    "\n",
    "Bi-directional RNNs\n",
    "- information flows in both directions, from path and future, independently. The computation are different in both direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9341f9",
   "metadata": {},
   "source": [
    "# LSTM & Name-Entity Recognition\n",
    "\n",
    "RNN \n",
    "- capture dependencies within a short range\n",
    "- Take up less RAM than any other n-grams models\n",
    "\n",
    "Downsides\n",
    "- struggle to capture long dependencies \n",
    "- exploading and vanishing gradients. \n",
    "\n",
    "Solutions to vanishing and exploading gradient problems\n",
    "- identity RNN and Relu activation - initialise identity matrix as weights \n",
    "- Gradient Cliping - fix max value for each element in the matrix \n",
    "- skip connection - skip a few hidden states and connect directly to the far away hidden states \n",
    "\n",
    "\n",
    "Gates in LSTM\n",
    "1. forget gate - information that is no longer important, decides which  to forget\n",
    "2. input gate - information to be stored, decides which to keep\n",
    "3. output gate - information to use at a current step, decide the current hidden state \n",
    "\n",
    "Note that there is the cell state and the hidden state, and then there is the output state. The forget gate is the first activation in the drawing above. It makes use of the previous hidden state h<t0> and the input x<t1>. The input gate makes use of the next two activations, the sigmoid and the tanh. Finally the output gate makes use of the last activation and the tanh right above it. This is just an overview of the architecture, we will dive into the details once we introduce the equations. \n",
    "\n",
    "Application of NER\n",
    "- search engine efficiency \n",
    "- recomendation engines\n",
    "- customer service\n",
    "- automatic trading (web scraping and sentiment classification)\n",
    "\n",
    "\n",
    "Training an NER system: \n",
    "1. Create a tensor for each input and its corresponding number \n",
    "2.  Put them in a batch ==> 64, 128, 256, 512 ...\n",
    "3. Feed it into an LSTM unit\n",
    "4. Run the output through a dense layer\n",
    "5. Predict using a log softmax over K classes (choose logsoftmax over softmax to provide better `numerical performance and gradient optimisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af342b0d",
   "metadata": {},
   "source": [
    "# Siamese Networks\n",
    "\n",
    "teachnique used to identify duplicate questions. \n",
    "\n",
    "Siamese networks made up of two identical networks which are merged at the end. \n",
    "\n",
    "Applications\n",
    "1. Handwritten checks\n",
    "2. Question duplicates \n",
    "3. Queries\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. Question -> embedding -> LSTM (could be any other architecture) -> Vector (output)\n",
    "2. Run this architecture again on another question. \n",
    "3. apply cosine similarity on both vector outputs\n",
    "4. set a threshold, anything smaller than the treshold is considered not identical while anything bigger is considered identical. \n",
    "\n",
    "One thing to remember is that sub-networks share identical parameters. This means that you only need to train one set of weights and not two. \n",
    "\n",
    "### Triplet Loss function\n",
    "Three sentences ~ anchor denoted $A$, postive denoted $P$ and negative denoted as  $N$\n",
    "\n",
    "$$ cos(v_1, v_2)  = \\frac {v_1 \\cdot v_2}{ |v_1| | v_2| }$$\n",
    "\n",
    "Minimizing \n",
    "$$ -cos(A, P) +  cos(A, N) \\leq 0$$\n",
    "\n",
    "Only need to prepare similar sentences for training because non-similar sentences will be the other sentences in the same batch. The diagonal line corresponds to scores of similar sentences, (normally they should be positive). The off-diagonals correspond to cosine scores between the anchor and the negative examples. \n",
    "\n",
    "Hard Negative Mining \n",
    "- mean negative ~ speed up training through noise reduction\n",
    "- closeset negative ~ help penalise the cost more (seperate the duplicates far away from non-duplicates)\n",
    "- mean negative mining \"taking an average of several example\"reduces the noise of the indidual sentence, assume noise to be a distribution centered around 0. average of several sentence is usually zero\n",
    "- closest negative \"taking the most simmilar word in term of similarity cost from the non-duplicates(off-diagonal values)\n",
    "\n",
    "$$ L_1 = max(mean\\_neg - N(A, P) + \\alpha, 0)  \\tag{1}$$\n",
    "$$ L_2 = max(closest\\_neg - N(A, p) + \\alpha, 0) \\tag{2} $$\n",
    "$$ L_{full} = L_1 + L_2 \\tag{3}$$\n",
    "\n",
    "the use of RElU activation function is to solve the \n",
    "- Semi-hard negative triplet:  cos(A,N) < cos(A,P) < cos(A,N) + 𝜶 \n",
    "- Hard negative triplet: cos(A,P) < cos(A,N)\n",
    "\n",
    "When non-duplicates are classified as duplicates in order words, their cosine similiarity bigger than the original duplicate. \n",
    "\n",
    "The key reason for one shot learning is to be able to classify new classes without retraining any models. You only need to learn the similarity function once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c0839",
   "metadata": {},
   "source": [
    "## Attention Models\n",
    "\n",
    "#### Neural Machine Translation\n",
    "\n",
    "NMT models with attention has four important components\n",
    "1. encoder (consits of embeddings and LSTM layers)\n",
    "2. pre-attention decoder  (consits of embeddings and LSTM layers and inputs are being shifted to the right to add start token)\n",
    "3. attention layer (scaled dot-product atention)\n",
    "4. post-attention decoder (LSTM, dense and logsoftmax)\n",
    "\n",
    "inputs are being processed by encoder and pre-attention decoder simultaneously  \n",
    "\n",
    "system evaluation\n",
    "- BLEU score (Bilingual Evaluation Understudy ) range between 0 and 1. closer to one the better\n",
    "- BLEU doest consider the semantic meaning \n",
    "- BLEU dosnt consider the sentence structure (focus more on words individually)\n",
    "\n",
    "- ROUGE-N to estimate the recall quality of machine translation \n",
    "- we can calcualte f1-score by combining both BLUE (precision) and ROUGE-N (recall) matrics together using the f1 score formula \n",
    "\n",
    "#### ways to construct sentences \n",
    "- greedy decoding and randiom sampling \n",
    "- greedign decoding ~ select the most probable word at each step but the best word at each step may not be optimal for longer sequences \n",
    "- provide probability for each word and sample accordingly for the next output (probelm with this is that it could be too random). Tempreture is a variable which can be adjusted to control how much randomness is in predictions.\n",
    "\n",
    "\n",
    "## Text Summarization with transformers\n",
    "- Issues with RNN \n",
    "    1. not efficient, No parllel computing because computation is done sequentially meaning that in order to compute the hidden state of t you need to already compute the precious t-1 hiddent state.\n",
    "    2. Loss of information for long sequences. For example, it is harder to keep track of whether the subject is singular or plural as you move further away from the subject.\n",
    "    3. Vanishing gradient problem for long sequences when you back-propagate, the gradients can become really small and as a result,  your model will not be learning much. \n",
    "\n",
    "Attention is all you need\n",
    "- self-attention mechanism in the encoder provides contextual representation of each item in the input sequence\n",
    "- maksed self-attention and encoder-decoder attention are in the decoder \n",
    "- positional encoding which encodes each input position in a sequence ()\n",
    "- in masked self-attention, queries cannot attend to the future\n",
    "\n",
    "Transformers are one of the most of the versatile deep learning model. here are some of example of the applications\n",
    "1. NLP applications, text summarzation, auto complete, name entity recognition, question answering, translation, chat-bots, text classification, etc\n",
    "\n",
    "#### Scaled and Dot-Product Attention\n",
    "1. the devision by the dimension is used to improve computational performance\n",
    "Scaled and Dot-Product Attention is the heart and soul of transformers\n",
    "the inputs of attention layer are queries, keys and values\n",
    "\n",
    "\n",
    "#### multi head attention\n",
    "- the number of times attention is applied is equal to the number of multi head attention\n",
    "\n",
    "## Question&Answering with Pre-Training Model\n",
    "Transfer learning - speed ups training process. instead of initialising the weight from scratch, we initialise the weights of the pre-trained model and training the weight on a new set of dataset. improve predictions, small datasets. \n",
    "1. feature-base (use the vector representation of the  pre-trained model and use it to fed into another model that solves a spesific task)\n",
    "2. finetuning (tweaking weights to solve spesific problem you are solving)\n",
    "\n",
    "General purpose learning. \n",
    "1. CBOW (given context words, predict the target word)\n",
    "\n",
    "general rule of thumb, the bigger the data, the bigger the model and the better the performance\n",
    "\n",
    "pre-training = domain adaptation. use unlabelled dataset to train the model (masked language modelling and next sentence prediction)\n",
    "There are two steps in the **BERT framework**: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks.  For fine tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks\n",
    "\n",
    "BERT Objective \n",
    "1. The input embeddings is the sum of position  embeddings + segment embeddings (split the sentence for next sentence prediction) + token embeddings\n",
    "2. for the segment embeddings, you can generalise this idea to do a bunch of other nlp tasks. for example segmenting the two part into question&answering, text&classification, article&summary, hyphothesis&premise etc\n",
    "\n",
    "T5 (text to text transformer)\n",
    "1. pre-training through MLM\n",
    "\n",
    "HuggingFace Pipeline function do three things\n",
    "1. preprocessing the iput\n",
    "2. running a model\n",
    "3. post-processing the output\n",
    "\n",
    "\n",
    "\n",
    "### Chatbox\n",
    "Transformers with long text sequences. \n",
    "if one tries to run a long sequence of text on a transformer, they might just run out of memory. there are two main parts the contribute to this\n",
    "1. attention sequence of length $L $ takes $ L^2 $ time and memory. intuition, you need each word to be compared with any other words in the sequence to capture the contextual meaning so $ L^2 $ comparisions are needed. \n",
    "2. N layesrs take N time as much  memory \n",
    "\n",
    "as bigger a model is, a more memory it requires to store actiavtion for backpropragation. \n",
    "\n",
    "Locality senstive hashing reduces the computational cost of fuinding k nearest neighbors\n",
    "\n",
    "The reformer allows you to fit up to 1 million tokens on a single 16 gigabyte GPU. It is designed to handle context windows of up to 1 million words. It combines two techniques to solve the problems of attention and memory allocation which are bottlenecks for the transformer networks.\n",
    "\n",
    "Reformer uses locality sensitive hashing, which you saw earlier in this specialization, to reduce the complexity of attending over long sequences. It also uses reversible residual layers to more efficiently use the memory available. In the picture below you can see how the reformer performs when compared to a normal full-attention model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb500394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.995732273553991"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a9c71197a12097caaa94dbd6926b7c26d52a7c73c772ca71a63d19ef0206318"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
