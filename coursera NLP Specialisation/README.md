# Contents

**Probabilistic Models**
1. Auto Correct
    - n-edit distance
    - minimum edit distance (dynamic programming)
    - edits (replace, insert, switch, delete)

2. Part of speech Tagging
    - Hidden Markov Chains
    - Populating transition matrix & emission matrix
    - Vertibi algorithm (initialisation, forward pass & backward pass) to find the highest probability POS tagging in a sentence

3. N-grams autocomplete language model
    - N-Grams and probabilities
    - Approximate sentence probability from N-Grams
    - Build a language model from a corpus
    - Fix missing information
    - Out of vocabulary words with <UNK>
    - Missing N-Gram in corpus with smoothing, backoff and interpolation
    - Evaluate language model with perplexity

4. Words embedding
    - Identify the key concepts of word representations
    - Generate word embeddings
    - Prepare text for machine learning
    - Implement the continuous bag-of-words model
